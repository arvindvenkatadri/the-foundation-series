---
title: Basics of Machine Learning - Classification
subtitle: 
date: "2021-08-09"
external_link: ""
image:
  caption: Photo by rawpixel on Unsplash
  focal_point: Smart
weight: 2
links:
# - icon: twitter
#   icon_pack: fab
#   name: Follow
#   url: https://twitter.com/arvind_v
slides:
summary: We will look at the basic models for Classification of Data.
tags:
- Machine Learning
- Orange Data Mining
- Decision Trees
- Random Forests
- 20 Questions Game
# url_code: "code/course-related/example/example.html"
# url_pdf: ""
# url_slides: "slides/new/index.html"
# url_video: ""
---
## A Childhood Game

Have you played **20 Questions**?

- How does one create questions in the game?  
  - Categories?
  - Numbers? How?
  - Comparisons?

- What sort of answers can you expect for each question?

- Can you imagine how the **20 Questions** Game can be shown as a **tree**? 

## What is a Decision Tree?

```{r echo=FALSE}
library(DiagrammeR)
grViz("
digraph tree_diagram {

  # a 'graph' statement
  graph [overlap = false, fontsize = 10]

  # several 'node' statements
  node [shape = circle,
        fontname = Helvetica,
        label = root]
  A; 

  node [shape = circle,
        fixedsize = true,
        label = nodes,
        fillcolor = purple,
        width = 0.9] // sets as circles
  B; C; 

node [shape = diamond,
        fixedsize = true,
        label = leaf,
        color = springgreen,
        width = 0.9] 
  D;E;

  # several 'edge' statements
  A->B[color = blue, label = yes]; A->C[color = red,label = no]; 
B->D[color = blue,label = yes]; B->E[color = red,label = no]
}
")
```

## Twenty times 20 Questions !!

What if we played **many** 20 Questions games in parallel? There would be many things to guess and there would be 20 questions aimed at guessing each of them. Would it be possible to play this "parallel" game? No?

Let us try!!

Let us look at the now famous `penguins` dataset, available here:

https://raw.githubusercontent.com/mwaskom/seaborn-data/master/penguins.csv

We see that there three `species` of penguins, that live on three `islands`. The measurements for each penguin are `flipper_length_mm`, `bill_depth_mm` and  `body_mass_g`. 

1. **Task 1:** Create a few data visualizations for the variables, and pairs of variables from this dataset. 

2. **Task 2:** Can you imagine how each of this dataset can be used in a `20 Questions Game`, to create a **Decision Tree** for this dataset as shown below?

![Penguins!](penguins.png)


#### What did we learn? 

- The 20Q Game can be viewed as a "Decision Tree" of Questions and Answers, 
- Each **fork** in the game is a Question. 
- Depending upon whether the current answer is **yes or no**, we turn in one direction or the other.
- Each of our questions is **based** on the information available in one or other of the columns!!
- We arrive at a final "answer" or "target" after a particular sequence of yes/no answers. This is the one of the **leaf** nodes in the Tree. 
- The `island` and the `species` columns are **categories** and are especially suited to being the **targets** for a 20 Questions Game. 
- We can therefore use an **entire column** of data as our 20Questions **target**, rather than just one entity, person. 

This is how we will use this Game as a Model for our first ML algorithm, **classification**.

## How do we Use our Decision Tree

Our aim is to make `predictions`. Predictions of what? Either a **numerical value** or a **class/category**. Trees can do both these things for us.

What we need to do is to use some data to **design the tree** and then use the tree to predict some aspect of the remaining, but similar, data. 

Let us split the `penguins` dataset into two pieces: a `training set` to design our tree, and a `test set` to check how it is working.

Download this [penguin tree file](penguins-tree.ows) and open it in Orange.


## How Many Trees do we Need? Enter the Wild Wood...

Even when we have **multiple targets**, we will see that the Questions, answers and resulting Decision Trees look very similar! Check all your individual Trees: do they ask the same Questions? Do they fork in the same way?

Yes, they all seem to use the same set of parameters to reach the target. So they are capable of being "biased" and make the **same mistakes**. So we ask: Does it help to use more than one tree, if all the questions/forks in the Trees are similar?

No...we need different Trees to be able to **ask different questions**, based on different **variables** or **features** in the data. That will make the Trees as different as possible and so...unbiased. 

How do we do this?

By tossing a coin to "randomize" the Trees into using different **features** at different times. 

A forest of such trees is called ~~**the Wild Wood**~~ **a Random Forest** ! 


## A Visual Introduction to Random Forests

In the Random Forest method, we do as follows:

1. Split the dataset into `training` and `test` subsets (70::30 proportion is very common). Keep aside the `testing` dataset for final testing.
2. Decide on a number of trees, say 100-500 in the forest. 
2. Take the training dataset and repeatedly sample some of the rows in it. Rows can be **repeated** too; this is called **bootstrap sampling**.
3. Give this sampled training set to each tree. Each tree develops a question from this dataset, in a **random fashion**, using a **randomly chosen** variable. 
E.g. with `penguins`, if our target is `species`, then some trees will will use `island`, some others will use `body_mass_g` and some others may use `bill_length_mm`. Use the answer to grow each tree separately. 
Since the questions are possibly based on a different variable at each time, the trees will grow in very different ways. 
4. Stop when the required accuracy has been achieved (the sets contain observations/rows from only one `species` predominantly)
5. With the `test set` let each tree vote on which `species` it has decided upon. Take the majority vote. 

Phew!!

Let's get a visual sense of how all this works:

https://waternova.github.io/random-forest-viz/


## Random Forest Classification for Heart Patients

Do you want to develop an ML model for heart patients? We have a dataset of heart patients at the [**University of California, ~~Arvind~~ Irvine ML Dataset Repository**](https://archive.ics.uci.edu/ml/datasets/heart+disease)

[**Heart Patient Data**](https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data). Import into Orange !!

What are the variables?

1. (age): age in years     
2. (sex): 1 = male; 0 = female   
3. (cp): chest-pain type( 4 types, 1/2/3/4)      
4. (trestbps): resting blood pressure (in mm Hg on admission to the hospital)
5. (chol) : serum cholestoral in mg/dl
6. (fbs): (fasting blood sugar > 120 mg/dl)  (1 = true; 0 = false)
7. (restecg): resting electrocardiographic results (0 = normal; 1= ST-T wave abnormality; 3 = LV hypertrophy)
8. (thalach): maximum heart rate achieved
9. (exang): exercise induced angina (1 = yes; 0 = no) (Puneet Rajkumar)  
10. (oldpeak): ST depression induced by exercise relative to rest 
11. (slope): the slope of the peak exercise ST segment
        -- Value 1: upsloping
        -- Value 2: flat
        -- Value 3: downsloping 
12. (ca): number of major vessels (0-3) colored by flourosopy 
13. (thal): 3 = normal; 6 = fixed defect; 7 = reversable defect   
14. (num) : **the target attribute**, diagnosis of heart disease (angiographic disease status)
        -- Value 0: < 50% diameter narrowing
        -- Value 1: > 50% diameter narrowing
        (in any major vessel: attributes 59 through 68 are vessels)




## How good is my Random Forest?

- Gini Impurity
- Cross Entropy
- Classification Error


## References


1. https://towardsdatascience.com/data-science-made-easy-data-modeling-and-prediction-using-orange-f451f17061fa

2. The beauty of Random Forests: https://orangedatamining.com/blog/2016/12/22/the-beauty-of-random-forest/

3. Pythagorean Trees for Random Forests: https://orangedatamining.com/blog/2016/07/29/pythagorean-trees-and-forests/

