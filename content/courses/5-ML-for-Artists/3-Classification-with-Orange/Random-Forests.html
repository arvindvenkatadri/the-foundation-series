---
title: "Random Forests"
author: "Arvind Venkatadri"
date: "13/06/2020"
output:
  html_document:
    df_print: paged
    toc: yes
    code_download: TRUE
editor_options: 
  chunk_output_type: inline
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="references" class="section level2">
<h2>References:</h2>
<ol style="list-style-type: decimal">
<li>Machine Learning Basics - Random Forest at <a href="https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/">Shirin’s Playground</a></li>
<li></li>
</ol>
</div>
<div id="penguin-random-forest-model-withrandomforest" class="section level2">
<h2>Penguin Random Forest Model with<code>randomForest</code></h2>
<p>Using the <code>penguins</code> dataset and Random Forest Classification.</p>
<pre class="r"><code>penguins</code></pre>
<pre><code>## # A tibble: 344 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ... with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;</code></pre>
<pre class="r"><code>summary(penguins)</code></pre>
<pre><code>##       species          island    bill_length_mm  bill_depth_mm  
##  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  
##  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  
##  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  
##                                  Mean   :43.92   Mean   :17.15  
##                                  3rd Qu.:48.50   3rd Qu.:18.70  
##                                  Max.   :59.60   Max.   :21.50  
##                                  NA&#39;s   :2       NA&#39;s   :2      
##  flipper_length_mm  body_mass_g       sex           year     
##  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  
##  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  
##  Median :197.0     Median :4050   NA&#39;s  : 11   Median :2008  
##  Mean   :200.9     Mean   :4202                Mean   :2008  
##  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  
##  Max.   :231.0     Max.   :6300                Max.   :2009  
##  NA&#39;s   :2         NA&#39;s   :2</code></pre>
<pre class="r"><code>penguins %&gt;% skimr::skim()</code></pre>
<table>
<caption><span id="tab:penguins">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">344</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">species</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Ade: 152, Gen: 124, Chi: 68</td>
</tr>
<tr class="even">
<td align="left">island</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Bis: 168, Dre: 124, Tor: 52</td>
</tr>
<tr class="odd">
<td align="left">sex</td>
<td align="right">11</td>
<td align="right">0.97</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">mal: 168, fem: 165</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bill_length_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">43.92</td>
<td align="right">5.46</td>
<td align="right">32.1</td>
<td align="right">39.23</td>
<td align="right">44.45</td>
<td align="right">48.5</td>
<td align="right">59.6</td>
<td align="left">▃▇▇▆▁</td>
</tr>
<tr class="even">
<td align="left">bill_depth_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">17.15</td>
<td align="right">1.97</td>
<td align="right">13.1</td>
<td align="right">15.60</td>
<td align="right">17.30</td>
<td align="right">18.7</td>
<td align="right">21.5</td>
<td align="left">▅▅▇▇▂</td>
</tr>
<tr class="odd">
<td align="left">flipper_length_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">200.92</td>
<td align="right">14.06</td>
<td align="right">172.0</td>
<td align="right">190.00</td>
<td align="right">197.00</td>
<td align="right">213.0</td>
<td align="right">231.0</td>
<td align="left">▂▇▃▅▂</td>
</tr>
<tr class="even">
<td align="left">body_mass_g</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">4201.75</td>
<td align="right">801.95</td>
<td align="right">2700.0</td>
<td align="right">3550.00</td>
<td align="right">4050.00</td>
<td align="right">4750.0</td>
<td align="right">6300.0</td>
<td align="left">▃▇▆▃▂</td>
</tr>
<tr class="odd">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2008.03</td>
<td align="right">0.82</td>
<td align="right">2007.0</td>
<td align="right">2007.00</td>
<td align="right">2008.00</td>
<td align="right">2009.0</td>
<td align="right">2009.0</td>
<td align="left">▇▁▇▁▇</td>
</tr>
</tbody>
</table>
<pre class="r"><code>penguins &lt;- penguins %&gt;% tidyr::drop_na()
# Spent one hour trying to find `drop-na()` ( 14 June 2020)</code></pre>
<pre class="r"><code># library(corrplot)
cor &lt;- penguins %&gt;% select(is.numeric) %&gt;% cor() </code></pre>
<pre><code>## Warning: Predicate functions must be wrapped in `where()`.
## 
##   # Bad
##   data %&gt;% select(is.numeric)
## 
##   # Good
##   data %&gt;% select(where(is.numeric))
## 
## i Please update your code.
## This message is displayed once per session.</code></pre>
<pre class="r"><code>cor %&gt;% corrplot(., method = &quot;ellipse&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/EDA%20on%20penguins%20data-1.png" width="672" /></p>
<pre class="r"><code># try these too:
# cor %&gt;% corrplot(., method = &quot;square&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)
# cor %&gt;% corrplot(., method = &quot;color&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)
# cor %&gt;% corrplot(., method = &quot;shade&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)</code></pre>
<p>Notes:
- <code>flipper_length_mm</code> and <code>culmen_depth_mm</code> are negtively correlated at approx (-0.7)
- <code>flipper_length_mm</code> and <code>body_mass_g</code> are positively correlated at approx 0.8</p>
<p>So we will use steps in the recipe to remove correlated variables.</p>
<div id="penguin-data-sampling-and-recipe" class="section level3">
<h3>Penguin Data Sampling and Recipe</h3>
<pre class="r"><code># Data Split
penguin_split &lt;- initial_split(penguins, prop = 0.6)
penguin_train &lt;- training(penguin_split)
penguin_test &lt;- testing(penguin_split)
penguin_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;199/134/333&gt;</code></pre>
<pre class="r"><code>head(penguin_train)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex  
##   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;int&gt;       &lt;int&gt; &lt;fct&gt;
## 1 Adelie  Biscoe           41            20                203        4725 male 
## 2 Chinst~ Dream            51.4          19                201        3950 male 
## 3 Adelie  Torge~           45.8          18.9              197        4150 male 
## 4 Adelie  Biscoe           37.7          16                183        3075 fema~
## 5 Adelie  Dream            34            17.1              185        3400 fema~
## 6 Adelie  Biscoe           41.1          18.2              192        4050 male 
## # ... with 1 more variable: year &lt;int&gt;</code></pre>
<pre class="r"><code># Recipe
penguin_recipe &lt;- penguins %&gt;% 
  recipe(species ~ .) %&gt;% 
  step_normalize(all_numeric()) %&gt;% # Scaling and Centering
  step_corr(all_numeric()) %&gt;%  # Handling correlated variables
  prep()

# Baking the data
penguin_train_baked &lt;-  penguin_train %&gt;% 
  bake(object = penguin_recipe, new_data = .)

penguin_test_baked &lt;-  penguin_test %&gt;% 
  bake(object = penguin_recipe, new_data = .)

head(penguin_train_baked)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex      year
##   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;
## 1 Biscoe         -0.547        1.44            0.145        0.643  male   1.18  
## 2 Dream           1.35         0.932           0.00236     -0.319  male   1.18  
## 3 Torge~          0.330        0.881          -0.283       -0.0709 male  -0.0517
## 4 Biscoe         -1.15        -0.592          -1.28        -1.41   fema~  1.18  
## 5 Dream          -1.83        -0.0329         -1.14        -1.00   fema~ -0.0517
## 6 Biscoe         -0.529        0.526          -0.640       -0.195  male  -0.0517
## # ... with 1 more variable: species &lt;fct&gt;</code></pre>
</div>
<div id="penguin-random-forest-model" class="section level3">
<h3>Penguin Random Forest Model</h3>
<pre class="r"><code>penguin_model &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_engine(&quot;randomForest&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)
penguin_model</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   trees = 100
## 
## Computational engine: randomForest</code></pre>
<pre class="r"><code>penguin_fit &lt;- 
  penguin_model %&gt;% 
  fit(species ~ .,penguin_train_baked)
penguin_fit</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  20ms 
## 
## Call:
##  randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) 
##                Type of random forest: classification
##                      Number of trees: 100
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 0.5%
## Confusion matrix:
##           Adelie Chinstrap Gentoo class.error
## Adelie        95         0      0  0.00000000
## Chinstrap      1        38      0  0.02564103
## Gentoo         0         0     65  0.00000000</code></pre>
<pre class="r"><code># iris_ranger &lt;- 
#   rand_forest(trees = 100) %&gt;% 
#   set_mode(&quot;classification&quot;) %&gt;% 
#   set_engine(&quot;ranger&quot;) %&gt;% 
#   fit(Species ~ ., data = iris_training_baked)</code></pre>
</div>
<div id="metrics-for-the-penguin-random-forest-model" class="section level3">
<h3>Metrics for the Penguin Random Forest Model</h3>
<pre class="r"><code># Predictions
predict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  
  dplyr::bind_cols(penguin_test_baked) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 134
## Columns: 9
## $ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, ~
## $ bill_length_mm    &lt;dbl&gt; -0.8215515, -0.8581235, -0.8764095, -1.3518452, -1.1~
## $ bill_depth_mm     &lt;dbl&gt; 0.11940428, 1.74440040, 1.23658911, 0.32252879, 0.72~
## $ flipper_length_mm &lt;dbl&gt; -1.06786655, -0.78247365, -0.42573251, -1.13921478, ~
## $ body_mass_g       &lt;dbl&gt; -0.5055254, -0.6918109, 0.5811398, -0.6297157, -1.31~
## $ sex               &lt;fct&gt; female, male, male, female, female, female, male, fe~
## $ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2~
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~</code></pre>
<pre class="r"><code># Prediction Accuracy Metrics
predict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  
  dplyr::bind_cols(penguin_test_baked) %&gt;% 
  yardstick::metrics(truth = species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.993
## 2 kap      multiclass     0.988</code></pre>
<pre class="r"><code># Prediction Probabilities
penguin_fit_probs &lt;- 
  predict(penguin_fit, penguin_test_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(penguin_test_baked)
glimpse(penguin_fit_probs)</code></pre>
<pre><code>## Rows: 134
## Columns: 11
## $ .pred_Adelie      &lt;dbl&gt; 0.97, 1.00, 0.90, 0.97, 1.00, 0.87, 0.96, 0.97, 0.95~
## $ .pred_Chinstrap   &lt;dbl&gt; 0.03, 0.00, 0.06, 0.03, 0.00, 0.13, 0.04, 0.03, 0.04~
## $ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.00, 0.04, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01~
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, ~
## $ bill_length_mm    &lt;dbl&gt; -0.8215515, -0.8581235, -0.8764095, -1.3518452, -1.1~
## $ bill_depth_mm     &lt;dbl&gt; 0.11940428, 1.74440040, 1.23658911, 0.32252879, 0.72~
## $ flipper_length_mm &lt;dbl&gt; -1.06786655, -0.78247365, -0.42573251, -1.13921478, ~
## $ body_mass_g       &lt;dbl&gt; -0.5055254, -0.6918109, 0.5811398, -0.6297157, -1.31~
## $ sex               &lt;fct&gt; female, male, male, female, female, female, male, fe~
## $ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2~
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~</code></pre>
<pre class="r"><code># Confusion Matrix
penguin_fit$fit$confusion %&gt;% tidy()</code></pre>
<pre><code>## Warning: &#39;tidy.numeric&#39; is deprecated.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>## Warning: `data_frame()` was deprecated in tibble 1.1.0.
## Please use `tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## # A tibble: 3 x 1
##   x[,&quot;Adelie&quot;] [,&quot;Chinstrap&quot;] [,&quot;Gentoo&quot;] [,&quot;class.error&quot;]
##          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;
## 1           95              0           0           0     
## 2            1             38           0           0.0256
## 3            0              0          65           0</code></pre>
<pre class="r"><code># Gain Curves
penguin_fit_probs %&gt;% 
  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Penguin%20Model%20Predictions%20and%20Metrics-1.png" width="672" /></p>
<pre class="r"><code># ROC Plot
penguin_fit_probs%&gt;%
  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Penguin%20Model%20Predictions%20and%20Metrics-2.png" width="672" />
### Using <code>broom</code> on the penguin model</p>
<pre class="r"><code>penguin_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;199/134/333&gt;</code></pre>
<pre class="r"><code>penguin_split %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 333 x 2
##      Row Data    
##    &lt;int&gt; &lt;chr&gt;   
##  1     1 Analysis
##  2     3 Analysis
##  3     4 Analysis
##  4     6 Analysis
##  5     8 Analysis
##  6     9 Analysis
##  7    10 Analysis
##  8    12 Analysis
##  9    13 Analysis
## 10    14 Analysis
## # ... with 323 more rows</code></pre>
<pre class="r"><code>penguin_recipe %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 2 x 6
##   number operation type      trained skip  id             
##    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          
## 1      1 step      normalize TRUE    FALSE normalize_vSVvd
## 2      2 step      corr      TRUE    FALSE corr_4ZHNL</code></pre>
<pre class="r"><code># Following do not work for `random forest models` !! ;-()
#penguin_model %&gt;% tidy()
#penguin_fit %&gt;% tidy() 
penguin_model %&gt;% str()</code></pre>
<pre><code>## List of 5
##  $ args    :List of 3
##   ..$ mtry : language ~NULL
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##   ..$ trees: language ~100
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##   ..$ min_n: language ~NULL
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  $ eng_args: Named list()
##   ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;quosures&quot; &quot;list&quot;
##  $ mode    : chr &quot;classification&quot;
##  $ method  : NULL
##  $ engine  : chr &quot;randomForest&quot;
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;rand_forest&quot; &quot;model_spec&quot;</code></pre>
<pre class="r"><code>penguin_test_baked</code></pre>
<pre><code>## # A tibble: 134 x 8
##    island  bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex    year
##    &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;
##  1 Torger~         -0.822        0.119            -1.07       -0.506 fema~ -1.28
##  2 Torger~         -0.858        1.74             -0.782      -0.692 male  -1.28
##  3 Torger~         -0.876        1.24             -0.426       0.581 male  -1.28
##  4 Torger~         -1.35         0.323            -1.14       -0.630 fema~ -1.28
##  5 Biscoe          -1.11         0.729            -2.07       -1.31  fema~ -1.28
##  6 Dream           -1.39        -0.0837           -0.426      -1.10  fema~ -1.28
##  7 Dream           -0.950        1.44             -0.782      -0.319 male  -1.28
##  8 Dream           -1.17         1.08             -1.42       -1.13  fema~ -1.28
##  9 Dream           -0.767        0.983            -1.21        0.550 male  -1.28
## 10 Dream           -0.584        0.627            -0.426      -0.381 male  -1.28
## # ... with 124 more rows, and 1 more variable: species &lt;fct&gt;</code></pre>
</div>
</div>
<div id="iris-random-forest-model-with-ranger" class="section level2">
<h2>Iris Random Forest Model with <code>ranger</code></h2>
<p>Using the <code>iris</code> dataset and Random Forest Classification.
This part uses <code>rsample</code> to split the data and the <code>recipes</code> to <em>prep</em> the data for model making.</p>
<pre class="r"><code>#set.seed(100)
iris_split &lt;- rsample::initial_split(iris, prop = 0.6)
iris_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;90/60/150&gt;</code></pre>
<pre class="r"><code>iris_split %&gt;% training() %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 90
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 6.5, 4.7, 5.5, 7.2, 5.8, 6.8, 5.1, 6.3, 6.6, 4.9, 4.9, 4.~
## $ Sepal.Width  &lt;dbl&gt; 3.0, 3.2, 2.4, 3.0, 2.8, 3.0, 3.7, 2.3, 3.0, 2.4, 3.0, 3.~
## $ Petal.Length &lt;dbl&gt; 5.8, 1.3, 3.7, 5.8, 5.1, 5.5, 1.5, 4.4, 4.4, 3.3, 1.4, 1.~
## $ Petal.Width  &lt;dbl&gt; 2.2, 0.2, 1.0, 1.6, 2.4, 2.1, 0.4, 1.3, 1.4, 1.0, 0.2, 0.~
## $ Species      &lt;fct&gt; virginica, setosa, versicolor, virginica, virginica, virg~</code></pre>
<pre class="r"><code>iris_split %&gt;% testing() %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 5.1, 4.6, 4.9, 4.8, 4.8, 5.1, 5.7, 5.1, 5.4, 5.1, 4.8, 4.~
## $ Sepal.Width  &lt;dbl&gt; 3.5, 3.4, 3.1, 3.4, 3.0, 3.5, 3.8, 3.8, 3.4, 3.3, 3.4, 3.~
## $ Petal.Length &lt;dbl&gt; 1.4, 1.4, 1.5, 1.6, 1.4, 1.4, 1.7, 1.5, 1.7, 1.7, 1.9, 1.~
## $ Petal.Width  &lt;dbl&gt; 0.2, 0.3, 0.1, 0.2, 0.1, 0.3, 0.3, 0.3, 0.2, 0.5, 0.2, 0.~
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~</code></pre>
<div id="iris-data-pre-processing-creating-the-recipe" class="section level3">
<h3>Iris Data Pre-Processing: Creating the Recipe</h3>
<p>The <code>recipes</code> package provides an interface that specializes in <em>data pre-processing</em>. Within the package, the functions that start, or execute, the data transformations are named after <strong>cooking actions</strong>. That makes the interface more user-friendly. For example:</p>
<ul>
<li><p><code>recipe()</code> - Starts a new set of transformations to be applied, similar to the <code>ggplot()</code> command. Its main argument is the model’s <code>formula</code>.</p></li>
<li><p><code>prep()</code> - Executes the transformations on top of the data that is supplied (<strong>typically, the training data</strong>). Each data transformation is a <code>step()</code> function. ( Recall what we did with the <code>caret</code> package: <em>Centering, Scaling, Removing Correlated variables</em>…)</p></li>
</ul>
<p>Note that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the <strong>train_tbl</strong> only. <a href="https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c" class="uri">https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c</a>
CRAN: The idea is that the preprocessing operations will all be <strong>created</strong> using the <em>training set</em> and then these steps will be <strong>applied</strong> to both the training and test set.</p>
<pre class="r"><code># Pre Processing the Training Data

iris_recipe &lt;- 
  training(iris_split) %&gt;% # Note: Using TRAINING data !!
  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!

# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).</code></pre>
<p>Q: How does the recipe “figure” out which are the outcomes and which are the predictors?
A.The <code>recipe</code> command defines <code>Outcomes</code> and <code>Predictors</code> using the formula interface. <del>Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…</del></p>
<p>Q. Why is the recipe not agnostic to data set? Is that a meaningful question?
A. The use of the <code>training set</code> in the recipe command is just to declare the variables and specify the <code>roles</code> of the data, nothing else. <code>Roles</code> are open-ended and extensible.
From <a href="https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html" class="uri">https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html</a> :</p>
<blockquote>
<p>This document demonstrates some basic uses of recipes. First, some definitions are required:
- <strong>variables</strong> are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y.
- <strong>roles</strong> define how variables will be used in the model. Examples are: <code>predictor</code> (independent variables), <code>response</code>, and <code>case weight</code>. This is meant to be open-ended and extensible.
- <strong>terms</strong> are columns in a <strong>design matrix</strong> such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of <code>principal components</code> or a set of columns, that define a <code>basis function</code> for a variable. These are synonymous with <code>features</code> in machine learning. Variables that have <code>predictor</code> roles would automatically be main <code>effect terms</code>.</p>
</blockquote>
<pre class="r"><code># Apply the transformation steps
iris_recipe &lt;- iris_recipe %&gt;% 
  step_corr(all_predictors()) %&gt;% 
  step_center(all_predictors(), -all_outcomes()) %&gt;% 
  step_scale(all_predictors(), -all_outcomes()) %&gt;% 
  prep()</code></pre>
<p>This has created the <code>recipe()</code> and prepped it too.
We now need to apply it to our datasets:</p>
<ul>
<li>Take <code>training</code> data and <code>bake()</code> it to prepare it for modelling.</li>
<li>Do the same for the <code>testing</code> set.</li>
</ul>
<pre class="r"><code>iris_training_baked &lt;- 
  iris_split %&gt;% 
  training() %&gt;% 
  bake(iris_recipe,.)
iris_training_baked</code></pre>
<pre><code>## # A tibble: 90 x 4
##    Sepal.Length Sepal.Width Petal.Width Species   
##           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     
##  1       0.850       -0.105       1.31  virginica 
##  2      -1.42         0.345      -1.33  setosa    
##  3      -0.410       -1.46       -0.277 versicolor
##  4       1.73        -0.105       0.514 virginica 
##  5      -0.0322      -0.555       1.57  virginica 
##  6       1.23        -0.105       1.17  virginica 
##  7      -0.915        1.47       -1.07  setosa    
##  8       0.598       -1.68        0.119 versicolor
##  9       0.976       -0.105       0.251 versicolor
## 10      -1.17        -1.46       -0.277 versicolor
## # ... with 80 more rows</code></pre>
<pre class="r"><code>iris_testing_baked &lt;- 
  iris_split %&gt;% 
  testing() %&gt;% 
  bake(iris_recipe,.)
iris_testing_baked </code></pre>
<pre><code>## # A tibble: 60 x 4
##    Sepal.Length Sepal.Width Petal.Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  
##  1       -0.915       1.02       -1.33  setosa 
##  2       -1.55        0.795      -1.20  setosa 
##  3       -1.17        0.120      -1.46  setosa 
##  4       -1.29        0.795      -1.33  setosa 
##  5       -1.29       -0.105      -1.46  setosa 
##  6       -0.915       1.02       -1.20  setosa 
##  7       -0.158       1.70       -1.20  setosa 
##  8       -0.915       1.70       -1.20  setosa 
##  9       -0.537       0.795      -1.33  setosa 
## 10       -0.915       0.570      -0.936 setosa 
## # ... with 50 more rows</code></pre>
</div>
<div id="iris-model-training-using-parsnip" class="section level3">
<h3>Iris Model Training using <code>parsnip</code></h3>
<p>Different ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The <code>tidymodels</code> package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.</p>
<p>The <code>parsnip</code> package is a successor to <code>caret</code>.</p>
<p>To model with <code>parsnip</code>:
1. Pick a <code>model</code> :
2. Set the <code>engine</code>
3. Set the <code>mode</code> (if needed): <em>Classification</em> or <em>Regression</em></p>
<p>Check <a href="https://tidymodels.github.io/parsnip/articles/articles/Models.html">here</a> for models available in <code>parsnip</code>.</p>
<ul>
<li><p>Mode: <em>classification</em> and <em>regression</em> in <code>parsnip</code>, each using a variety of models. ( <strong>Which Way</strong>). This defines the form of the output.</p></li>
<li><p>Engine: The <code>engine</code> is the <strong>R package</strong> that is invoked by <code>parsnip</code> to execute the model. E.g <code>glm</code>, <code>glmnet</code>,<code>keras</code>.( <strong>How</strong> ) <code>parsnip</code> provides <strong>wrappers</strong> for models from these packages.</p></li>
<li><p>Model: is the <strong>specific technique</strong> used for the modelling task. E.g <code>linear_reg()</code>, <code>logistic_reg()</code>, <code>mars</code>, <code>decision_tree</code>, <code>nearest_neighbour</code>…(What model).</p></li>
</ul>
<p>and models have:
- <code>hyperparameters</code>: that are numerical or factor variables that <code>tune</code> the model ( Like the alpha beta parameters for Bayesian priors)</p>
<p>We can use the <code>random forest</code> model to <strong>classify</strong> the iris into species. Here Species is the <code>Outcome</code> variable and the rest are <code>predictor</code> variables. The <code>random forest</code> model is provided by the <code>ranger</code> package, to which <code>tidymodels/parsnip</code> provides a simple and consistent interface.</p>
<pre class="r"><code>library(ranger)
iris_ranger &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;ranger&quot;) %&gt;% 
  fit(Species ~ ., data = iris_training_baked)</code></pre>
<p><code>ranger</code> can generate random forest models for <code>classification</code>, <code>regression</code>, <code>survival</code>( time series, time to event stuff). <code>Extreme Forests</code> are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with <code>feature bagging</code>.
We can also run the same model using the <code>randomForest</code> package:</p>
<pre class="r"><code>library(randomForest,quietly = TRUE)</code></pre>
<pre><code>## randomForest 4.6-14</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ranger&#39;:
## 
##     importance</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre class="r"><code>iris_rf &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;randomForest&quot;) %&gt;% 
  fit(Species ~ ., data = iris_training_baked)</code></pre>
</div>
<div id="iris-predictions" class="section level3">
<h3>Iris Predictions</h3>
<p>The <code>predict()</code> function run against a <code>parsnip</code> model returns a prediction <code>tibble</code>. By default, the prediction variable is called <code>.pred_class</code>.</p>
<pre class="r"><code>predict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%  
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~
## $ Sepal.Length &lt;dbl&gt; -0.9147569, -1.5451407, -1.1669104, -1.2929872, -1.292987~
## $ Sepal.Width  &lt;dbl&gt; 1.0204716, 0.7953676, 0.1200555, 0.7953676, -0.1050486, 1~
## $ Petal.Width  &lt;dbl&gt; -1.3316103, -1.1997677, -1.4634529, -1.3316103, -1.463452~
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~</code></pre>
</div>
<div id="iris-classification-model-validation" class="section level3">
<h3>Iris Classification Model Validation</h3>
<p>We use <code>metrics()</code> function from the <code>yardstick</code> package to evaluate how good the model is.</p>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked) %&gt;%
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  yardstick::metrics(truth = Species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.883
## 2 kap      multiclass     0.824</code></pre>
<p>We can also check the metrics for <code>randomForest</code> model:</p>
<pre class="r"><code>predict(iris_rf, iris_testing_baked) %&gt;%
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  yardstick::metrics(truth = Species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.867
## 2 kap      multiclass     0.798</code></pre>
</div>
<div id="iris-per-classifier-metrics" class="section level3">
<h3>Iris Per-Classifier Metrics</h3>
<p>We can use the parameter <code>type = "prob"</code> in the <code>predict()</code> function to obtain a probability score on each prediction.
<strong>TBD: How is this prob calculated?</strong> Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for <em>all possible outcomes</em> for <strong>each test datum</strong>)</p>
<pre class="r"><code>iris_ranger_probs &lt;- 
  predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(iris_testing_baked)
glimpse(iris_ranger_probs)</code></pre>
<pre><code>## Rows: 60
## Columns: 7
## $ .pred_setosa     &lt;dbl&gt; 0.99777778, 0.99375000, 0.97090476, 0.98708333, 0.952~
## $ .pred_versicolor &lt;dbl&gt; 0.002222222, 0.000000000, 0.022873016, 0.006666667, 0~
## $ .pred_virginica  &lt;dbl&gt; 0.000000000, 0.006250000, 0.006222222, 0.006250000, 0~
## $ Sepal.Length     &lt;dbl&gt; -0.9147569, -1.5451407, -1.1669104, -1.2929872, -1.29~
## $ Sepal.Width      &lt;dbl&gt; 1.0204716, 0.7953676, 0.1200555, 0.7953676, -0.105048~
## $ Petal.Width      &lt;dbl&gt; -1.3316103, -1.1997677, -1.4634529, -1.3316103, -1.46~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>iris_rf_probs &lt;- 
  predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(iris_testing_baked)
glimpse(iris_rf_probs)</code></pre>
<pre><code>## Rows: 60
## Columns: 7
## $ .pred_setosa     &lt;dbl&gt; 1.00, 0.99, 0.98, 0.98, 0.95, 1.00, 0.91, 1.00, 0.99,~
## $ .pred_versicolor &lt;dbl&gt; 0.00, 0.00, 0.02, 0.01, 0.05, 0.00, 0.08, 0.00, 0.00,~
## $ .pred_virginica  &lt;dbl&gt; 0.00, 0.01, 0.00, 0.01, 0.00, 0.00, 0.01, 0.00, 0.01,~
## $ Sepal.Length     &lt;dbl&gt; -0.9147569, -1.5451407, -1.1669104, -1.2929872, -1.29~
## $ Sepal.Width      &lt;dbl&gt; 1.0204716, 0.7953676, 0.1200555, 0.7953676, -0.105048~
## $ Petal.Width      &lt;dbl&gt; -1.3316103, -1.1997677, -1.4634529, -1.3316103, -1.46~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code># Tabulating the probabilities
ftable(iris_rf_probs$.pred_versicolor)</code></pre>
<pre><code>##   0 0.01 0.02 0.03 0.04 0.05 0.08 0.09 0.12 0.13 0.17 0.25 0.35 0.39 0.41 0.43 0.47 0.73 0.82 0.83 0.87 0.88 0.89 0.91 0.92 0.95 0.96 0.97
##                                                                                                                                           
##  15    5    5    2    2    4    2    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1    1    1    1    3    1    2</code></pre>
<pre class="r"><code>ftable(iris_rf_probs$.pred_virginica)</code></pre>
<pre><code>##   0 0.01 0.03 0.04 0.05 0.08 0.09 0.11 0.12 0.13 0.15 0.17 0.18 0.44 0.48 0.51 0.58 0.65 0.75 0.78 0.83 0.87 0.88 0.92 0.93 0.95 0.96 0.97 0.98
##                                                                                                                                                
##  14    7    3    1    3    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    1    2    1    2    3    3    3</code></pre>
<pre class="r"><code>ftable(iris_rf_probs$.pred_setosa)</code></pre>
<pre><code>##   0 0.02 0.03 0.05 0.06 0.07 0.08 0.11 0.12 0.13 0.15 0.91 0.95 0.96 0.97 0.98 0.99  1
##                                                                                       
##  24    6    1    1    1    1    1    1    1    1    1    1    2    1    1    5    4  7</code></pre>
<pre><code>
### Iris Classifier: Gain and ROC Curves

We can plot gain and ROC curves for each of these models


```r
iris_ranger_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 134
## Columns: 5
## $ .level          &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;set~
## $ .n              &lt;dbl&gt; 0, 2, 4, 5, 6, 7, 9, 11, 12, 14, 15, 17, 18, 19, 20, 2~
## $ .n_events       &lt;dbl&gt; 0, 2, 4, 5, 6, 7, 9, 11, 12, 14, 15, 17, 18, 19, 20, 2~
## $ .percent_tested &lt;dbl&gt; 0.000000, 3.333333, 6.666667, 8.333333, 10.000000, 11.~
## $ .percent_found  &lt;dbl&gt; 0.00000, 9.52381, 19.04762, 23.80952, 28.57143, 33.333~</code></pre>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60ranger%60-1.png" width="672" /></p>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 137
## Columns: 4
## $ .level      &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;~
## $ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001000000, 0.001111111, 0.002111111, ~
## $ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.2051282, 0.2307692, 0.3333333, 0.3~
## $ sensitivity &lt;dbl&gt; 1.000000, 1.000000, 1.000000, 1.000000, 1.000000, 1.000000~</code></pre>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60ranger%60-2.png" width="672" /></p>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 78
## Columns: 5
## $ .level          &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;set~
## $ .n              &lt;dbl&gt; 0, 7, 11, 16, 17, 18, 20, 21, 22, 23, 24, 25, 26, 27, ~
## $ .n_events       &lt;dbl&gt; 0, 7, 11, 16, 17, 18, 20, 21, 21, 21, 21, 21, 21, 21, ~
## $ .percent_tested &lt;dbl&gt; 0.000000, 11.666667, 18.333333, 26.666667, 28.333333, ~
## $ .percent_found  &lt;dbl&gt; 0.00000, 33.33333, 52.38095, 76.19048, 80.95238, 85.71~</code></pre>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60randomForest%60-1.png" width="672" /></p>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 81
## Columns: 4
## $ .level      &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;~
## $ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.02, 0.03, 0.05, 0.06, 0.07, 0.08, 0.11, 0.12~
## $ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.6153846, 0.7692308, 0.7948718, 0.8~
## $ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0~</code></pre>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60randomForest%60-2.png" width="672" /></p>
</div>
<div id="iris-classifier-metrics" class="section level3">
<h3>Iris Classifier: Metrics</h3>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_setosa     &lt;dbl&gt; 0.99777778, 0.99375000, 0.97090476, 0.98708333, 0.952~
## $ .pred_versicolor &lt;dbl&gt; 0.002222222, 0.000000000, 0.022873016, 0.006666667, 0~
## $ .pred_virginica  &lt;dbl&gt; 0.000000000, 0.006250000, 0.006222222, 0.006250000, 0~
## $ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy    multiclass     0.883
## 2 kap         multiclass     0.824
## 3 mn_log_loss multiclass     0.275
## 4 roc_auc     hand_till      0.975</code></pre>
<pre class="r"><code># And for the `randomForest`method

predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_setosa     &lt;dbl&gt; 1.00, 0.99, 0.98, 0.98, 0.95, 1.00, 0.91, 1.00, 0.99,~
## $ .pred_versicolor &lt;dbl&gt; 0.00, 0.00, 0.02, 0.01, 0.05, 0.00, 0.08, 0.00, 0.00,~
## $ .pred_virginica  &lt;dbl&gt; 0.00, 0.01, 0.00, 0.01, 0.00, 0.00, 0.01, 0.00, 0.01,~
## $ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy    multiclass     0.883
## 2 kap         multiclass     0.824
## 3 mn_log_loss multiclass     0.248
## 4 roc_auc     hand_till      0.980</code></pre>
</div>
</div>
