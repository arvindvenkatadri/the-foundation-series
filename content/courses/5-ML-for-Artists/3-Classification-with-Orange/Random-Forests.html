---
title: "Random Forests"
author: "Arvind Venkatadri"
date: "13/06/2020"
output:
  html_document:
    df_print: paged
    toc: yes
    code_download: TRUE
editor_options: 
  chunk_output_type: inline
---



<div id="references" class="section level2">
<h2>References:</h2>
<ol style="list-style-type: decimal">
<li>Machine Learning Basics - Random Forest at <a href="https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/">Shirin’s Playground</a></li>
<li></li>
</ol>
</div>
<div id="penguin-random-forest-model-withrandomforest" class="section level2">
<h2>Penguin Random Forest Model with<code>randomForest</code></h2>
<p>Using the <code>penguins</code> dataset and Random Forest Classification.</p>
<pre class="r"><code>penguins</code></pre>
<pre><code>## # A tibble: 344 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ... with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;</code></pre>
<pre class="r"><code>summary(penguins)</code></pre>
<pre><code>##       species          island    bill_length_mm  bill_depth_mm  
##  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  
##  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  
##  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  
##                                  Mean   :43.92   Mean   :17.15  
##                                  3rd Qu.:48.50   3rd Qu.:18.70  
##                                  Max.   :59.60   Max.   :21.50  
##                                  NA&#39;s   :2       NA&#39;s   :2      
##  flipper_length_mm  body_mass_g       sex           year     
##  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  
##  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  
##  Median :197.0     Median :4050   NA&#39;s  : 11   Median :2008  
##  Mean   :200.9     Mean   :4202                Mean   :2008  
##  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  
##  Max.   :231.0     Max.   :6300                Max.   :2009  
##  NA&#39;s   :2         NA&#39;s   :2</code></pre>
<pre class="r"><code>penguins %&gt;% skimr::skim()</code></pre>
<table>
<caption><span id="tab:penguins">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">344</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<colgroup>
<col width="16%" />
<col width="12%" />
<col width="16%" />
<col width="9%" />
<col width="10%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">species</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Ade: 152, Gen: 124, Chi: 68</td>
</tr>
<tr class="even">
<td align="left">island</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Bis: 168, Dre: 124, Tor: 52</td>
</tr>
<tr class="odd">
<td align="left">sex</td>
<td align="right">11</td>
<td align="right">0.97</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">mal: 168, fem: 165</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="13%" />
<col width="7%" />
<col width="10%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="5%" />
<col width="30%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bill_length_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">43.92</td>
<td align="right">5.46</td>
<td align="right">32.1</td>
<td align="right">39.23</td>
<td align="right">44.45</td>
<td align="right">48.5</td>
<td align="right">59.6</td>
<td align="left">▃▇▇▆▁</td>
</tr>
<tr class="even">
<td align="left">bill_depth_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">17.15</td>
<td align="right">1.97</td>
<td align="right">13.1</td>
<td align="right">15.60</td>
<td align="right">17.30</td>
<td align="right">18.7</td>
<td align="right">21.5</td>
<td align="left">▅▅▇▇▂</td>
</tr>
<tr class="odd">
<td align="left">flipper_length_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">200.92</td>
<td align="right">14.06</td>
<td align="right">172.0</td>
<td align="right">190.00</td>
<td align="right">197.00</td>
<td align="right">213.0</td>
<td align="right">231.0</td>
<td align="left">▂▇▃▅▂</td>
</tr>
<tr class="even">
<td align="left">body_mass_g</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">4201.75</td>
<td align="right">801.95</td>
<td align="right">2700.0</td>
<td align="right">3550.00</td>
<td align="right">4050.00</td>
<td align="right">4750.0</td>
<td align="right">6300.0</td>
<td align="left">▃▇▆▃▂</td>
</tr>
<tr class="odd">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2008.03</td>
<td align="right">0.82</td>
<td align="right">2007.0</td>
<td align="right">2007.00</td>
<td align="right">2008.00</td>
<td align="right">2009.0</td>
<td align="right">2009.0</td>
<td align="left">▇▁▇▁▇</td>
</tr>
</tbody>
</table>
<pre class="r"><code>penguins &lt;- penguins %&gt;% tidyr::drop_na()
# Spent one hour trying to find `drop-na()` ( 14 June 2020)</code></pre>
<pre class="r"><code># library(corrplot)
cor &lt;- penguins %&gt;% select(is.numeric) %&gt;% cor() </code></pre>
<pre><code>## Warning: Predicate functions must be wrapped in `where()`.
## 
##   # Bad
##   data %&gt;% select(is.numeric)
## 
##   # Good
##   data %&gt;% select(where(is.numeric))
## 
## i Please update your code.
## This message is displayed once per session.</code></pre>
<pre class="r"><code>cor %&gt;% corrplot(., method = &quot;ellipse&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/EDA%20on%20penguins%20data-1.png" width="672" /></p>
<pre class="r"><code># try these too:
# cor %&gt;% corrplot(., method = &quot;square&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)
# cor %&gt;% corrplot(., method = &quot;color&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)
# cor %&gt;% corrplot(., method = &quot;shade&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)</code></pre>
<p>Notes:
- <code>flipper_length_mm</code> and <code>culmen_depth_mm</code> are negtively correlated at approx (-0.7)
- <code>flipper_length_mm</code> and <code>body_mass_g</code> are positively correlated at approx 0.8</p>
<p>So we will use steps in the recipe to remove correlated variables.</p>
<div id="penguin-data-sampling-and-recipe" class="section level3">
<h3>Penguin Data Sampling and Recipe</h3>
<pre class="r"><code># Data Split
penguin_split &lt;- initial_split(penguins, prop = 0.6)
penguin_train &lt;- training(penguin_split)
penguin_test &lt;- testing(penguin_split)
penguin_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;199/134/333&gt;</code></pre>
<pre class="r"><code>head(penguin_train)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex  
##   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;int&gt;       &lt;int&gt; &lt;fct&gt;
## 1 Chinst~ Dream            49            19.5              210        3950 male 
## 2 Gentoo  Biscoe           48.5          14.1              220        5300 male 
## 3 Adelie  Biscoe           42.2          19.5              197        4275 male 
## 4 Adelie  Dream            40.8          18.4              195        3900 male 
## 5 Chinst~ Dream            46.9          16.6              192        2700 fema~
## 6 Adelie  Dream            37.3          16.8              192        3000 fema~
## # ... with 1 more variable: year &lt;int&gt;</code></pre>
<pre class="r"><code># Recipe
penguin_recipe &lt;- penguins %&gt;% 
  recipe(species ~ .) %&gt;% 
  step_normalize(all_numeric()) %&gt;% # Scaling and Centering
  step_corr(all_numeric()) %&gt;%  # Handling correlated variables
  prep()

# Baking the data
penguin_train_baked &lt;-  penguin_train %&gt;% 
  bake(object = penguin_recipe, new_data = .)

penguin_test_baked &lt;-  penguin_test %&gt;% 
  bake(object = penguin_recipe, new_data = .)

head(penguin_train_baked)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex      year
##   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;
## 1 Dream           0.916         1.19             0.644     -0.319  male  -0.0517
## 2 Biscoe          0.824        -1.56             1.36       1.36   male  -0.0517
## 3 Biscoe         -0.328         1.19            -0.283      0.0844 male   1.18  
## 4 Dream          -0.584         0.627           -0.426     -0.381  male  -1.28  
## 5 Dream           0.532        -0.287           -0.640     -1.87   fema~ -0.0517
## 6 Dream          -1.22         -0.185           -0.640     -1.50   fema~  1.18  
## # ... with 1 more variable: species &lt;fct&gt;</code></pre>
</div>
<div id="penguin-random-forest-model" class="section level3">
<h3>Penguin Random Forest Model</h3>
<pre class="r"><code>penguin_model &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_engine(&quot;randomForest&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)
penguin_model</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   trees = 100
## 
## Computational engine: randomForest</code></pre>
<pre class="r"><code>penguin_fit &lt;- 
  penguin_model %&gt;% 
  fit(species ~ .,penguin_train_baked)
penguin_fit</code></pre>
<pre><code>## parsnip model object
## 
## 
## Call:
##  randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) 
##                Type of random forest: classification
##                      Number of trees: 100
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 0.5%
## Confusion matrix:
##           Adelie Chinstrap Gentoo class.error
## Adelie        85         1      0  0.01162791
## Chinstrap      0        41      0  0.00000000
## Gentoo         0         0     72  0.00000000</code></pre>
<pre class="r"><code># iris_ranger &lt;- 
#   rand_forest(trees = 100) %&gt;% 
#   set_mode(&quot;classification&quot;) %&gt;% 
#   set_engine(&quot;ranger&quot;) %&gt;% 
#   fit(Species ~ ., data = iris_training_baked)</code></pre>
</div>
<div id="metrics-for-the-penguin-random-forest-model" class="section level3">
<h3>Metrics for the Penguin Random Forest Model</h3>
<pre class="r"><code># Predictions
predict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  
  dplyr::bind_cols(penguin_test_baked) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 134
## Columns: 9
## $ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~
## $ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.6752636, -0.8581235, -0.8764095, -0.9~
## $ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.42409105, 1.74440040, 1.23658911, 0.93~
## $ flipper_length_mm &lt;dbl&gt; -1.42460769, -0.42573251, -0.78247365, -0.42573251, ~
## $ body_mass_g       &lt;dbl&gt; -0.56762058, -1.18857213, -0.69181089, 0.58113979, -~
## $ sex               &lt;fct&gt; male, female, male, male, female, male, female, fema~
## $ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2~
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~</code></pre>
<pre class="r"><code># Prediction Accuracy Metrics
predict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  
  dplyr::bind_cols(penguin_test_baked) %&gt;% 
  yardstick::metrics(truth = species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.963
## 2 kap      multiclass     0.941</code></pre>
<pre class="r"><code># Prediction Probabilities
penguin_fit_probs &lt;- 
  predict(penguin_fit, penguin_test_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(penguin_test_baked)
glimpse(penguin_fit_probs)</code></pre>
<pre><code>## Rows: 134
## Columns: 11
## $ .pred_Adelie      &lt;dbl&gt; 1.00, 0.98, 0.98, 0.98, 0.97, 0.82, 0.99, 0.98, 0.99~
## $ .pred_Chinstrap   &lt;dbl&gt; 0.00, 0.02, 0.02, 0.02, 0.03, 0.16, 0.01, 0.00, 0.00~
## $ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.00, 0.02, 0.01~
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~
## $ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.6752636, -0.8581235, -0.8764095, -0.9~
## $ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.42409105, 1.74440040, 1.23658911, 0.93~
## $ flipper_length_mm &lt;dbl&gt; -1.42460769, -0.42573251, -0.78247365, -0.42573251, ~
## $ body_mass_g       &lt;dbl&gt; -0.56762058, -1.18857213, -0.69181089, 0.58113979, -~
## $ sex               &lt;fct&gt; male, female, male, male, female, male, female, fema~
## $ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2~
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~</code></pre>
<pre class="r"><code># Confusion Matrix
penguin_fit$fit$confusion %&gt;% tidy()</code></pre>
<pre><code>## Warning: &#39;tidy.numeric&#39; is deprecated.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>## Warning: `data_frame()` was deprecated in tibble 1.1.0.
## Please use `tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## # A tibble: 3 x 1
##   x[,&quot;Adelie&quot;] [,&quot;Chinstrap&quot;] [,&quot;Gentoo&quot;] [,&quot;class.error&quot;]
##          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;
## 1           85              1           0           0.0116
## 2            0             41           0           0     
## 3            0              0          72           0</code></pre>
<pre class="r"><code># Gain Curves
penguin_fit_probs %&gt;% 
  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Penguin%20Model%20Predictions%20and%20Metrics-1.png" width="672" /></p>
<pre class="r"><code># ROC Plot
penguin_fit_probs%&gt;%
  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Penguin%20Model%20Predictions%20and%20Metrics-2.png" width="672" />
### Using <code>broom</code> on the penguin model</p>
<pre class="r"><code>penguin_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;199/134/333&gt;</code></pre>
<pre class="r"><code>penguin_split %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 333 x 2
##      Row Data    
##    &lt;int&gt; &lt;chr&gt;   
##  1     2 Analysis
##  2     4 Analysis
##  3     6 Analysis
##  4     8 Analysis
##  5     9 Analysis
##  6    10 Analysis
##  7    11 Analysis
##  8    15 Analysis
##  9    16 Analysis
## 10    17 Analysis
## # ... with 323 more rows</code></pre>
<pre class="r"><code>penguin_recipe %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 2 x 6
##   number operation type      trained skip  id             
##    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          
## 1      1 step      normalize TRUE    FALSE normalize_U0anA
## 2      2 step      corr      TRUE    FALSE corr_CvuzT</code></pre>
<pre class="r"><code># Following do not work for `random forest models` !! ;-()
#penguin_model %&gt;% tidy()
#penguin_fit %&gt;% tidy() 
penguin_model %&gt;% str()</code></pre>
<pre><code>## List of 5
##  $ args    :List of 3
##   ..$ mtry : language ~NULL
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##   ..$ trees: language ~100
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##   ..$ min_n: language ~NULL
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  $ eng_args: Named list()
##   ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;quosures&quot; &quot;list&quot;
##  $ mode    : chr &quot;classification&quot;
##  $ method  : NULL
##  $ engine  : chr &quot;randomForest&quot;
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;rand_forest&quot; &quot;model_spec&quot;</code></pre>
<pre class="r"><code>penguin_test_baked</code></pre>
<pre><code>## # A tibble: 134 x 8
##    island  bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex    year
##    &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;
##  1 Torger~         -0.895         0.780           -1.42       -0.568 male  -1.28
##  2 Torger~         -0.675         0.424           -0.426      -1.19  fema~ -1.28
##  3 Torger~         -0.858         1.74            -0.782      -0.692 male  -1.28
##  4 Torger~         -0.876         1.24            -0.426       0.581 male  -1.28
##  5 Torger~         -0.968         0.932           -0.426      -0.940 fema~ -1.28
##  6 Torger~         -0.273         1.80            -0.283       0.364 male  -1.28
##  7 Torger~         -1.75          0.627           -1.21       -1.10  fema~ -1.28
##  8 Biscoe          -1.48          1.03            -0.854      -0.506 fema~ -1.28
##  9 Biscoe          -1.06          0.475           -1.14       -0.319 male  -1.28
## 10 Biscoe          -0.639         0.881           -1.50       -0.319 male  -1.28
## # ... with 124 more rows, and 1 more variable: species &lt;fct&gt;</code></pre>
</div>
</div>
<div id="iris-random-forest-model-with-ranger" class="section level2">
<h2>Iris Random Forest Model with <code>ranger</code></h2>
<p>Using the <code>iris</code> dataset and Random Forest Classification.
This part uses <code>rsample</code> to split the data and the <code>recipes</code> to <em>prep</em> the data for model making.</p>
<pre class="r"><code>#set.seed(100)
iris_split &lt;- rsample::initial_split(iris, prop = 0.6)
iris_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;90/60/150&gt;</code></pre>
<pre class="r"><code>iris_split %&gt;% training() %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 90
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 6.3, 5.7, 5.0, 5.5, 5.6, 5.9, 5.5, 5.2, 5.7, 6.1, 7.2, 6.~
## $ Sepal.Width  &lt;dbl&gt; 2.9, 2.8, 3.3, 2.4, 2.8, 3.0, 2.5, 3.5, 3.0, 2.8, 3.0, 2.~
## $ Petal.Length &lt;dbl&gt; 5.6, 4.5, 1.4, 3.8, 4.9, 5.1, 4.0, 1.5, 4.2, 4.0, 5.8, 4.~
## $ Petal.Width  &lt;dbl&gt; 1.8, 1.3, 0.2, 1.1, 2.0, 1.8, 1.3, 0.2, 1.2, 1.3, 1.6, 1.~
## $ Species      &lt;fct&gt; virginica, versicolor, setosa, versicolor, virginica, vir~</code></pre>
<pre class="r"><code>iris_split %&gt;% testing() %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 4.9, 4.6, 5.4, 5.0, 5.4, 4.8, 4.8, 5.7, 5.1, 5.1, 5.4, 5.~
## $ Sepal.Width  &lt;dbl&gt; 3.0, 3.1, 3.9, 3.4, 3.7, 3.4, 3.0, 4.4, 3.5, 3.8, 3.4, 3.~
## $ Petal.Length &lt;dbl&gt; 1.4, 1.5, 1.7, 1.5, 1.5, 1.6, 1.4, 1.5, 1.4, 1.5, 1.7, 1.~
## $ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.4, 0.2, 0.2, 0.2, 0.1, 0.4, 0.3, 0.3, 0.2, 0.~
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~</code></pre>
<div id="iris-data-pre-processing-creating-the-recipe" class="section level3">
<h3>Iris Data Pre-Processing: Creating the Recipe</h3>
<p>The <code>recipes</code> package provides an interface that specializes in <em>data pre-processing</em>. Within the package, the functions that start, or execute, the data transformations are named after <strong>cooking actions</strong>. That makes the interface more user-friendly. For example:</p>
<ul>
<li><p><code>recipe()</code> - Starts a new set of transformations to be applied, similar to the <code>ggplot()</code> command. Its main argument is the model’s <code>formula</code>.</p></li>
<li><p><code>prep()</code> - Executes the transformations on top of the data that is supplied (<strong>typically, the training data</strong>). Each data transformation is a <code>step()</code> function. ( Recall what we did with the <code>caret</code> package: <em>Centering, Scaling, Removing Correlated variables</em>…)</p></li>
</ul>
<p>Note that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the <strong>train_tbl</strong> only. <a href="https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c" class="uri">https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c</a>
CRAN: The idea is that the preprocessing operations will all be <strong>created</strong> using the <em>training set</em> and then these steps will be <strong>applied</strong> to both the training and test set.</p>
<pre class="r"><code># Pre Processing the Training Data

iris_recipe &lt;- 
  training(iris_split) %&gt;% # Note: Using TRAINING data !!
  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!

# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).</code></pre>
<p>Q: How does the recipe “figure” out which are the outcomes and which are the predictors?
A.The <code>recipe</code> command defines <code>Outcomes</code> and <code>Predictors</code> using the formula interface. <del>Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…</del></p>
<p>Q. Why is the recipe not agnostic to data set? Is that a meaningful question?
A. The use of the <code>training set</code> in the recipe command is just to declare the variables and specify the <code>roles</code> of the data, nothing else. <code>Roles</code> are open-ended and extensible.
From <a href="https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html" class="uri">https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html</a> :</p>
<blockquote>
<p>This document demonstrates some basic uses of recipes. First, some definitions are required:
- <strong>variables</strong> are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y.
- <strong>roles</strong> define how variables will be used in the model. Examples are: <code>predictor</code> (independent variables), <code>response</code>, and <code>case weight</code>. This is meant to be open-ended and extensible.
- <strong>terms</strong> are columns in a <strong>design matrix</strong> such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of <code>principal components</code> or a set of columns, that define a <code>basis function</code> for a variable. These are synonymous with <code>features</code> in machine learning. Variables that have <code>predictor</code> roles would automatically be main <code>effect terms</code>.</p>
</blockquote>
<pre class="r"><code># Apply the transformation steps
iris_recipe &lt;- iris_recipe %&gt;% 
  step_corr(all_predictors()) %&gt;% 
  step_center(all_predictors(), -all_outcomes()) %&gt;% 
  step_scale(all_predictors(), -all_outcomes()) %&gt;% 
  prep()</code></pre>
<p>This has created the <code>recipe()</code> and prepped it too.
We now need to apply it to our datasets:</p>
<ul>
<li>Take <code>training</code> data and <code>bake()</code> it to prepare it for modelling.</li>
<li>Do the same for the <code>testing</code> set.</li>
</ul>
<pre class="r"><code>iris_training_baked &lt;- 
  iris_split %&gt;% 
  training() %&gt;% 
  bake(iris_recipe,.)
iris_training_baked</code></pre>
<pre><code>## # A tibble: 90 x 4
##    Sepal.Length Sepal.Width Petal.Width Species   
##           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     
##  1       0.414      -0.284       0.713  virginica 
##  2      -0.284      -0.528       0.0260 versicolor
##  3      -1.10        0.690      -1.49   setosa    
##  4      -0.517      -1.50       -0.249  versicolor
##  5      -0.401      -0.528       0.988  virginica 
##  6      -0.0517     -0.0406      0.713  virginica 
##  7      -0.517      -1.26        0.0260 versicolor
##  8      -0.866       1.18       -1.49   setosa    
##  9      -0.284      -0.0406     -0.111  versicolor
## 10       0.181      -0.528       0.0260 versicolor
## # ... with 80 more rows</code></pre>
<pre class="r"><code>iris_testing_baked &lt;- 
  iris_split %&gt;% 
  testing() %&gt;% 
  bake(iris_recipe,.)
iris_testing_baked </code></pre>
<pre><code>## # A tibble: 60 x 4
##    Sepal.Length Sepal.Width Petal.Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  
##  1       -1.21      -0.0406       -1.49 setosa 
##  2       -1.56       0.203        -1.49 setosa 
##  3       -0.633      2.15         -1.21 setosa 
##  4       -1.10       0.934        -1.49 setosa 
##  5       -0.633      1.66         -1.49 setosa 
##  6       -1.33       0.934        -1.49 setosa 
##  7       -1.33      -0.0406       -1.62 setosa 
##  8       -0.284      3.37         -1.21 setosa 
##  9       -0.982      1.18         -1.35 setosa 
## 10       -0.982      1.91         -1.35 setosa 
## # ... with 50 more rows</code></pre>
</div>
<div id="iris-model-training-using-parsnip" class="section level3">
<h3>Iris Model Training using <code>parsnip</code></h3>
<p>Different ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The <code>tidymodels</code> package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.</p>
<p>The <code>parsnip</code> package is a successor to <code>caret</code>.</p>
<p>To model with <code>parsnip</code>:
1. Pick a <code>model</code> :
2. Set the <code>engine</code>
3. Set the <code>mode</code> (if needed): <em>Classification</em> or <em>Regression</em></p>
<p>Check <a href="https://tidymodels.github.io/parsnip/articles/articles/Models.html">here</a> for models available in <code>parsnip</code>.</p>
<ul>
<li><p>Mode: <em>classification</em> and <em>regression</em> in <code>parsnip</code>, each using a variety of models. ( <strong>Which Way</strong>). This defines the form of the output.</p></li>
<li><p>Engine: The <code>engine</code> is the <strong>R package</strong> that is invoked by <code>parsnip</code> to execute the model. E.g <code>glm</code>, <code>glmnet</code>,<code>keras</code>.( <strong>How</strong> ) <code>parsnip</code> provides <strong>wrappers</strong> for models from these packages.</p></li>
<li><p>Model: is the <strong>specific technique</strong> used for the modelling task. E.g <code>linear_reg()</code>, <code>logistic_reg()</code>, <code>mars</code>, <code>decision_tree</code>, <code>nearest_neighbour</code>…(What model).</p></li>
</ul>
<p>and models have:
- <code>hyperparameters</code>: that are numerical or factor variables that <code>tune</code> the model ( Like the alpha beta parameters for Bayesian priors)</p>
<p>We can use the <code>random forest</code> model to <strong>classify</strong> the iris into species. Here Species is the <code>Outcome</code> variable and the rest are <code>predictor</code> variables. The <code>random forest</code> model is provided by the <code>ranger</code> package, to which <code>tidymodels/parsnip</code> provides a simple and consistent interface.</p>
<pre class="r"><code>library(ranger)
iris_ranger &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;ranger&quot;) %&gt;% 
  fit(Species ~ ., data = iris_training_baked)</code></pre>
<p><code>ranger</code> can generate random forest models for <code>classification</code>, <code>regression</code>, <code>survival</code>( time series, time to event stuff). <code>Extreme Forests</code> are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with <code>feature bagging</code>.
We can also run the same model using the <code>randomForest</code> package:</p>
<pre class="r"><code>library(randomForest,quietly = TRUE)</code></pre>
<pre><code>## randomForest 4.7-1</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ranger&#39;:
## 
##     importance</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre class="r"><code>iris_rf &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;randomForest&quot;) %&gt;% 
  fit(Species ~ ., data = iris_training_baked)</code></pre>
</div>
<div id="iris-predictions" class="section level3">
<h3>Iris Predictions</h3>
<p>The <code>predict()</code> function run against a <code>parsnip</code> model returns a prediction <code>tibble</code>. By default, the prediction variable is called <code>.pred_class</code>.</p>
<pre class="r"><code>predict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%  
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~
## $ Sepal.Length &lt;dbl&gt; -1.2146749, -1.5635709, -0.6331816, -1.0983763, -0.633181~
## $ Sepal.Width  &lt;dbl&gt; -0.04059063, 0.20295313, 2.15130320, 0.93358441, 1.664215~
## $ Petal.Width  &lt;dbl&gt; -1.4858413, -1.4858413, -1.2109683, -1.4858413, -1.485841~
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~</code></pre>
</div>
<div id="iris-classification-model-validation" class="section level3">
<h3>Iris Classification Model Validation</h3>
<p>We use <code>metrics()</code> function from the <code>yardstick</code> package to evaluate how good the model is.</p>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked) %&gt;%
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  yardstick::metrics(truth = Species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.933
## 2 kap      multiclass     0.898</code></pre>
<p>We can also check the metrics for <code>randomForest</code> model:</p>
<pre class="r"><code>predict(iris_rf, iris_testing_baked) %&gt;%
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  yardstick::metrics(truth = Species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.933
## 2 kap      multiclass     0.898</code></pre>
</div>
<div id="iris-per-classifier-metrics" class="section level3">
<h3>Iris Per-Classifier Metrics</h3>
<p>We can use the parameter <code>type = "prob"</code> in the <code>predict()</code> function to obtain a probability score on each prediction.
<strong>TBD: How is this prob calculated?</strong> Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for <em>all possible outcomes</em> for <strong>each test datum</strong>)</p>
<pre class="r"><code>iris_ranger_probs &lt;- 
  predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(iris_testing_baked)
glimpse(iris_ranger_probs)</code></pre>
<pre><code>## Rows: 60
## Columns: 7
## $ .pred_setosa     &lt;dbl&gt; 0.8217143, 0.8956111, 0.9146071, 0.9912778, 0.9263929~
## $ .pred_versicolor &lt;dbl&gt; 0.126829365, 0.068765873, 0.058468254, 0.001111111, 0~
## $ .pred_virginica  &lt;dbl&gt; 0.051456349, 0.035623016, 0.026924603, 0.007611111, 0~
## $ Sepal.Length     &lt;dbl&gt; -1.2146749, -1.5635709, -0.6331816, -1.0983763, -0.63~
## $ Sepal.Width      &lt;dbl&gt; -0.04059063, 0.20295313, 2.15130320, 0.93358441, 1.66~
## $ Petal.Width      &lt;dbl&gt; -1.4858413, -1.4858413, -1.2109683, -1.4858413, -1.48~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>iris_rf_probs &lt;- 
  predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(iris_testing_baked)
glimpse(iris_rf_probs)</code></pre>
<pre><code>## Rows: 60
## Columns: 7
## $ .pred_setosa     &lt;dbl&gt; 0.85, 0.98, 0.97, 1.00, 0.97, 0.98, 0.87, 0.94, 1.00,~
## $ .pred_versicolor &lt;dbl&gt; 0.12, 0.02, 0.02, 0.00, 0.02, 0.01, 0.11, 0.06, 0.00,~
## $ .pred_virginica  &lt;dbl&gt; 0.03, 0.00, 0.01, 0.00, 0.01, 0.01, 0.02, 0.00, 0.00,~
## $ Sepal.Length     &lt;dbl&gt; -1.2146749, -1.5635709, -0.6331816, -1.0983763, -0.63~
## $ Sepal.Width      &lt;dbl&gt; -0.04059063, 0.20295313, 2.15130320, 0.93358441, 1.66~
## $ Petal.Width      &lt;dbl&gt; -1.4858413, -1.4858413, -1.2109683, -1.4858413, -1.48~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code># Tabulating the probabilities
ftable(iris_rf_probs$.pred_versicolor)</code></pre>
<pre><code>##  0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.08 0.11 0.12 0.15 0.17 0.18 0.19 0.22 0.27 0.31 0.37 0.42 0.62 0.74 0.8 0.82 0.85 0.87 0.92 0.93 0.94 0.95 0.96 0.97 1
##                                                                                                                                                               
##  9    3    6    1    2    2    4    2    1    2    2    1    1    1    3    1    1    1    1    1    1    1   1    1    1    1    1    1    1    1    3    1 1</code></pre>
<pre class="r"><code>ftable(iris_rf_probs$.pred_virginica)</code></pre>
<pre><code>##   0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.12 0.13 0.18 0.26 0.38 0.5 0.56 0.69 0.72 0.81 0.82 0.83 0.85 0.88 0.91 0.92 0.94 0.96 0.98  1
##                                                                                                                                        
##  12    9    5    2    3    1    1    2    1    1    2    1    1   1    1    1    1    3    1    1    1    1    1    2    2    1    1  1</code></pre>
<pre class="r"><code>ftable(iris_rf_probs$.pred_setosa)</code></pre>
<pre><code>##   0 0.01 0.02 0.03 0.06 0.07 0.13 0.71 0.85 0.87 0.9 0.92 0.94 0.95 0.97 0.98 0.99  1
##                                                                                      
##  24    3    3    1    1    1    1    1    1    2   1    1    3    2    3    3    2  7</code></pre>
<pre><code>
### Iris Classifier: Gain and ROC Curves

We can plot gain and ROC curves for each of these models


```r
iris_ranger_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 149
## Columns: 5
## $ .level          &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;set~
## $ .n              &lt;dbl&gt; 0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~
## $ .n_events       &lt;dbl&gt; 0, 1, 2, 4, 5, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17~
## $ .percent_tested &lt;dbl&gt; 0.000000, 1.666667, 3.333333, 6.666667, 8.333333, 11.6~
## $ .percent_found  &lt;dbl&gt; 0.000000, 3.846154, 7.692308, 15.384615, 19.230769, 26~</code></pre>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60ranger%60-1.png" width="672" /></p>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 152
## Columns: 4
## $ .level      &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;~
## $ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001111111, 0.002222222, 0.002500000, ~
## $ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.2352941, 0.3235294, 0.3823529, 0.4~
## $ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0~</code></pre>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60ranger%60-2.png" width="672" /></p>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 82
## Columns: 5
## $ .level          &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;set~
## $ .n              &lt;dbl&gt; 0, 7, 9, 12, 15, 17, 20, 21, 22, 24, 25, 26, 27, 28, 2~
## $ .n_events       &lt;dbl&gt; 0, 7, 9, 12, 15, 17, 20, 21, 22, 24, 25, 26, 26, 26, 2~
## $ .percent_tested &lt;dbl&gt; 0.000000, 11.666667, 15.000000, 20.000000, 25.000000, ~
## $ .percent_found  &lt;dbl&gt; 0.000000, 26.923077, 34.615385, 46.153846, 57.692308, ~</code></pre>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60randomForest%60-1.png" width="672" /></p>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 85
## Columns: 4
## $ .level      &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;~
## $ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.02, 0.03, 0.06, 0.07, 0.13, 0.71, 0.85~
## $ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.7058824, 0.7941176, 0.8823529, 0.9~
## $ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0~</code></pre>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60randomForest%60-2.png" width="672" /></p>
</div>
<div id="iris-classifier-metrics" class="section level3">
<h3>Iris Classifier: Metrics</h3>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_setosa     &lt;dbl&gt; 0.8217143, 0.8956111, 0.9146071, 0.9912778, 0.9263929~
## $ .pred_versicolor &lt;dbl&gt; 0.126829365, 0.068765873, 0.058468254, 0.001111111, 0~
## $ .pred_virginica  &lt;dbl&gt; 0.051456349, 0.035623016, 0.026924603, 0.007611111, 0~
## $ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy    multiclass     0.933
## 2 kap         multiclass     0.898
## 3 mn_log_loss multiclass     0.201
## 4 roc_auc     hand_till      0.996</code></pre>
<pre class="r"><code># And for the `randomForest`method

predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_setosa     &lt;dbl&gt; 0.85, 0.98, 0.97, 1.00, 0.97, 0.98, 0.87, 0.94, 1.00,~
## $ .pred_versicolor &lt;dbl&gt; 0.12, 0.02, 0.02, 0.00, 0.02, 0.01, 0.11, 0.06, 0.00,~
## $ .pred_virginica  &lt;dbl&gt; 0.03, 0.00, 0.01, 0.00, 0.01, 0.01, 0.02, 0.00, 0.00,~
## $ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy    multiclass     0.933
## 2 kap         multiclass     0.898
## 3 mn_log_loss multiclass     0.172
## 4 roc_auc     hand_till      0.996</code></pre>
</div>
</div>
