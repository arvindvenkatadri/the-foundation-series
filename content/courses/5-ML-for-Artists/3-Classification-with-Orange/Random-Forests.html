---
title: "Random Forests"
author: "Arvind Venkatadri"
date: "13/06/2020"
output:
  html_document:
    df_print: paged
    toc: yes
    code_download: TRUE
editor_options: 
  chunk_output_type: inline
---

<script src="/rmarkdown-libs/header-attrs/header-attrs.js"></script>


<div id="references" class="section level2">
<h2>References:</h2>
<ol style="list-style-type: decimal">
<li>Machine Learning Basics - Random Forest at <a href="https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/">Shirin’s Playground</a></li>
<li></li>
</ol>
</div>
<div id="penguin-random-forest-model-withrandomforest" class="section level2">
<h2>Penguin Random Forest Model with<code>randomForest</code></h2>
<p>Using the <code>penguins</code> dataset and Random Forest Classification.</p>
<pre class="r"><code>penguins</code></pre>
<pre><code>## # A tibble: 344 x 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g
##    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750
##  2 Adelie  Torgersen           39.5          17.4               186        3800
##  3 Adelie  Torgersen           40.3          18                 195        3250
##  4 Adelie  Torgersen           NA            NA                  NA          NA
##  5 Adelie  Torgersen           36.7          19.3               193        3450
##  6 Adelie  Torgersen           39.3          20.6               190        3650
##  7 Adelie  Torgersen           38.9          17.8               181        3625
##  8 Adelie  Torgersen           39.2          19.6               195        4675
##  9 Adelie  Torgersen           34.1          18.1               193        3475
## 10 Adelie  Torgersen           42            20.2               190        4250
## # ... with 334 more rows, and 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;</code></pre>
<pre class="r"><code>summary(penguins)</code></pre>
<pre><code>##       species          island    bill_length_mm  bill_depth_mm  
##  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10  
##  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60  
##  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30  
##                                  Mean   :43.92   Mean   :17.15  
##                                  3rd Qu.:48.50   3rd Qu.:18.70  
##                                  Max.   :59.60   Max.   :21.50  
##                                  NA&#39;s   :2       NA&#39;s   :2      
##  flipper_length_mm  body_mass_g       sex           year     
##  Min.   :172.0     Min.   :2700   female:165   Min.   :2007  
##  1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  
##  Median :197.0     Median :4050   NA&#39;s  : 11   Median :2008  
##  Mean   :200.9     Mean   :4202                Mean   :2008  
##  3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  
##  Max.   :231.0     Max.   :6300                Max.   :2009  
##  NA&#39;s   :2         NA&#39;s   :2</code></pre>
<pre class="r"><code>penguins %&gt;% skimr::skim()</code></pre>
<table>
<caption><span id="tab:penguins">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">344</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">species</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Ade: 152, Gen: 124, Chi: 68</td>
</tr>
<tr class="even">
<td align="left">island</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Bis: 168, Dre: 124, Tor: 52</td>
</tr>
<tr class="odd">
<td align="left">sex</td>
<td align="right">11</td>
<td align="right">0.97</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">mal: 168, fem: 165</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bill_length_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">43.92</td>
<td align="right">5.46</td>
<td align="right">32.1</td>
<td align="right">39.23</td>
<td align="right">44.45</td>
<td align="right">48.5</td>
<td align="right">59.6</td>
<td align="left">▃▇▇▆▁</td>
</tr>
<tr class="even">
<td align="left">bill_depth_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">17.15</td>
<td align="right">1.97</td>
<td align="right">13.1</td>
<td align="right">15.60</td>
<td align="right">17.30</td>
<td align="right">18.7</td>
<td align="right">21.5</td>
<td align="left">▅▅▇▇▂</td>
</tr>
<tr class="odd">
<td align="left">flipper_length_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">200.92</td>
<td align="right">14.06</td>
<td align="right">172.0</td>
<td align="right">190.00</td>
<td align="right">197.00</td>
<td align="right">213.0</td>
<td align="right">231.0</td>
<td align="left">▂▇▃▅▂</td>
</tr>
<tr class="even">
<td align="left">body_mass_g</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">4201.75</td>
<td align="right">801.95</td>
<td align="right">2700.0</td>
<td align="right">3550.00</td>
<td align="right">4050.00</td>
<td align="right">4750.0</td>
<td align="right">6300.0</td>
<td align="left">▃▇▆▃▂</td>
</tr>
<tr class="odd">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2008.03</td>
<td align="right">0.82</td>
<td align="right">2007.0</td>
<td align="right">2007.00</td>
<td align="right">2008.00</td>
<td align="right">2009.0</td>
<td align="right">2009.0</td>
<td align="left">▇▁▇▁▇</td>
</tr>
</tbody>
</table>
<pre class="r"><code>penguins &lt;- penguins %&gt;% tidyr::drop_na()
# Spent one hour trying to find `drop-na()` ( 14 June 2020)</code></pre>
<pre class="r"><code># library(corrplot)
cor &lt;- penguins %&gt;% select(is.numeric) %&gt;% cor() </code></pre>
<pre><code>## Warning: Predicate functions must be wrapped in `where()`.
## 
##   # Bad
##   data %&gt;% select(is.numeric)
## 
##   # Good
##   data %&gt;% select(where(is.numeric))
## 
## i Please update your code.
## This message is displayed once per session.</code></pre>
<pre class="r"><code>cor %&gt;% corrplot(., method = &quot;ellipse&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/EDA%20on%20penguins%20data-1.png" width="672" /></p>
<pre class="r"><code># try these too:
# cor %&gt;% corrplot(., method = &quot;square&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)
# cor %&gt;% corrplot(., method = &quot;color&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)
# cor %&gt;% corrplot(., method = &quot;shade&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)</code></pre>
<p>Notes:
- <code>flipper_length_mm</code> and <code>culmen_depth_mm</code> are negtively correlated at approx (-0.7)
- <code>flipper_length_mm</code> and <code>body_mass_g</code> are positively correlated at approx 0.8</p>
<p>So we will use steps in the recipe to remove correlated variables.</p>
<div id="penguin-data-sampling-and-recipe" class="section level3">
<h3>Penguin Data Sampling and Recipe</h3>
<pre class="r"><code># Data Split
penguin_split &lt;- initial_split(penguins, prop = 0.6)
penguin_train &lt;- training(penguin_split)
penguin_test &lt;- testing(penguin_split)
penguin_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;199/134/333&gt;</code></pre>
<pre class="r"><code>head(penguin_train)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   species island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex  
##   &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;int&gt;       &lt;int&gt; &lt;fct&gt;
## 1 Adelie  Dream            36            17.9              190        3450 fema~
## 2 Adelie  Biscoe           38.1          16.5              198        3825 fema~
## 3 Gentoo  Biscoe           50.5          15.9              222        5550 male 
## 4 Gentoo  Biscoe           46.1          13.2              211        4500 fema~
## 5 Adelie  Dream            35.6          17.5              191        3175 fema~
## 6 Adelie  Torge~           40.2          17                176        3450 fema~
## # ... with 1 more variable: year &lt;int&gt;</code></pre>
<pre class="r"><code># Recipe
penguin_recipe &lt;- penguins %&gt;% 
  recipe(species ~ .) %&gt;% 
  step_normalize(all_numeric()) %&gt;% # Scaling and Centering
  step_corr(all_numeric()) %&gt;%  # Handling correlated variables
  prep()

# Baking the data
penguin_train_baked &lt;-  penguin_train %&gt;% 
  bake(object = penguin_recipe, new_data = .)

penguin_test_baked &lt;-  penguin_test %&gt;% 
  bake(object = penguin_recipe, new_data = .)

head(penguin_train_baked)</code></pre>
<pre><code>## # A tibble: 6 x 8
##   island bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex      year
##   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;   &lt;dbl&gt;
## 1 Dream          -1.46         0.373            -0.782      -0.940 fema~ -1.28  
## 2 Biscoe         -1.08        -0.338            -0.212      -0.474 fema~  1.18  
## 3 Biscoe          1.19        -0.642             1.50        1.67  male  -0.0517
## 4 Biscoe          0.385       -2.01              0.716       0.364 fema~ -1.28  
## 5 Dream          -1.53         0.170            -0.711      -1.28  fema~  1.18  
## 6 Torge~         -0.694       -0.0837           -1.78       -0.940 fema~  1.18  
## # ... with 1 more variable: species &lt;fct&gt;</code></pre>
</div>
<div id="penguin-random-forest-model" class="section level3">
<h3>Penguin Random Forest Model</h3>
<pre class="r"><code>penguin_model &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_engine(&quot;randomForest&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)
penguin_model</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   trees = 100
## 
## Computational engine: randomForest</code></pre>
<pre class="r"><code>penguin_fit &lt;- 
  penguin_model %&gt;% 
  fit(species ~ .,penguin_train_baked)
penguin_fit</code></pre>
<pre><code>## parsnip model object
## 
## Fit time:  20ms 
## 
## Call:
##  randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) 
##                Type of random forest: classification
##                      Number of trees: 100
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 1.51%
## Confusion matrix:
##           Adelie Chinstrap Gentoo class.error
## Adelie        88         1      0  0.01123596
## Chinstrap      2        37      0  0.05128205
## Gentoo         0         0     71  0.00000000</code></pre>
<pre class="r"><code># iris_ranger &lt;- 
#   rand_forest(trees = 100) %&gt;% 
#   set_mode(&quot;classification&quot;) %&gt;% 
#   set_engine(&quot;ranger&quot;) %&gt;% 
#   fit(Species ~ ., data = iris_training_baked)</code></pre>
</div>
<div id="metrics-for-the-penguin-random-forest-model" class="section level3">
<h3>Metrics for the Penguin Random Forest Model</h3>
<pre class="r"><code># Predictions
predict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  
  dplyr::bind_cols(penguin_test_baked) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 134
## Columns: 9
## $ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~
## $ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.6752636, -0.8764095, -0.9861254, -1.7~
## $ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.42409105, 1.23658911, 2.04908718, 1.99~
## $ flipper_length_mm &lt;dbl&gt; -1.4246077, -0.4257325, -0.4257325, -0.7111254, -0.2~
## $ body_mass_g       &lt;dbl&gt; -0.567620576, -1.188572125, 0.581139791, -0.50552542~
## $ sex               &lt;fct&gt; male, female, male, male, male, male, male, male, fe~
## $ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2~
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~</code></pre>
<pre class="r"><code># Prediction Accuracy Metrics
predict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  
  dplyr::bind_cols(penguin_test_baked) %&gt;% 
  yardstick::metrics(truth = species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.993
## 2 kap      multiclass     0.988</code></pre>
<pre class="r"><code># Prediction Probabilities
penguin_fit_probs &lt;- 
  predict(penguin_fit, penguin_test_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(penguin_test_baked)
glimpse(penguin_fit_probs)</code></pre>
<pre><code>## Rows: 134
## Columns: 11
## $ .pred_Adelie      &lt;dbl&gt; 1.00, 0.96, 0.96, 1.00, 0.99, 0.76, 1.00, 1.00, 0.98~
## $ .pred_Chinstrap   &lt;dbl&gt; 0.00, 0.04, 0.03, 0.00, 0.01, 0.23, 0.00, 0.00, 0.02~
## $ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.00, 0.01, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00~
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse~
## $ bill_length_mm    &lt;dbl&gt; -0.8946955, -0.6752636, -0.8764095, -0.9861254, -1.7~
## $ bill_depth_mm     &lt;dbl&gt; 0.77955895, 0.42409105, 1.23658911, 2.04908718, 1.99~
## $ flipper_length_mm &lt;dbl&gt; -1.4246077, -0.4257325, -0.4257325, -0.7111254, -0.2~
## $ body_mass_g       &lt;dbl&gt; -0.567620576, -1.188572125, 0.581139791, -0.50552542~
## $ sex               &lt;fct&gt; male, female, male, male, male, male, male, male, fe~
## $ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2~
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel~</code></pre>
<pre class="r"><code># Confusion Matrix
penguin_fit$fit$confusion %&gt;% tidy()</code></pre>
<pre><code>## Warning: &#39;tidy.numeric&#39; is deprecated.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>## Warning: `data_frame()` was deprecated in tibble 1.1.0.
## Please use `tibble()` instead.
## This warning is displayed once every 8 hours.
## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.</code></pre>
<pre><code>## # A tibble: 3 x 1
##   x[,&quot;Adelie&quot;] [,&quot;Chinstrap&quot;] [,&quot;Gentoo&quot;] [,&quot;class.error&quot;]
##          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;
## 1           88              1           0           0.0112
## 2            2             37           0           0.0513
## 3            0              0          71           0</code></pre>
<pre class="r"><code># Gain Curves
penguin_fit_probs %&gt;% 
  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Penguin%20Model%20Predictions%20and%20Metrics-1.png" width="672" /></p>
<pre class="r"><code># ROC Plot
penguin_fit_probs%&gt;%
  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Penguin%20Model%20Predictions%20and%20Metrics-2.png" width="672" />
### Using <code>broom</code> on the penguin model</p>
<pre class="r"><code>penguin_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;199/134/333&gt;</code></pre>
<pre class="r"><code>penguin_split %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 333 x 2
##      Row Data    
##    &lt;int&gt; &lt;chr&gt;   
##  1     2 Analysis
##  2     4 Analysis
##  3     5 Analysis
##  4     6 Analysis
##  5     8 Analysis
##  6    11 Analysis
##  7    12 Analysis
##  8    13 Analysis
##  9    14 Analysis
## 10    16 Analysis
## # ... with 323 more rows</code></pre>
<pre class="r"><code>penguin_recipe %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 2 x 6
##   number operation type      trained skip  id             
##    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          
## 1      1 step      normalize TRUE    FALSE normalize_9xN7M
## 2      2 step      corr      TRUE    FALSE corr_q4aMd</code></pre>
<pre class="r"><code># Following do not work for `random forest models` !! ;-()
#penguin_model %&gt;% tidy()
#penguin_fit %&gt;% tidy() 
penguin_model %&gt;% str()</code></pre>
<pre><code>## List of 5
##  $ args    :List of 3
##   ..$ mtry : language ~NULL
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##   ..$ trees: language ~100
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##   ..$ min_n: language ~NULL
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  $ eng_args: Named list()
##   ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;quosures&quot; &quot;list&quot;
##  $ mode    : chr &quot;classification&quot;
##  $ method  : NULL
##  $ engine  : chr &quot;randomForest&quot;
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;rand_forest&quot; &quot;model_spec&quot;</code></pre>
<pre class="r"><code>penguin_test_baked</code></pre>
<pre><code>## # A tibble: 134 x 8
##    island  bill_length_mm bill_depth_mm flipper_length_~ body_mass_g sex    year
##    &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;            &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt; &lt;dbl&gt;
##  1 Torger~         -0.895        0.780            -1.42     -0.568   male  -1.28
##  2 Torger~         -0.675        0.424            -0.426    -1.19    fema~ -1.28
##  3 Torger~         -0.876        1.24             -0.426     0.581   male  -1.28
##  4 Torger~         -0.986        2.05             -0.711    -0.506   male  -1.28
##  5 Torger~         -1.72         2.00             -0.212     0.240   male  -1.28
##  6 Torger~          0.367        2.20             -0.497    -0.00876 male  -1.28
##  7 Biscoe          -1.06         0.475            -1.14     -0.319   male  -1.28
##  8 Biscoe          -0.950        0.0178           -1.50     -0.506   male  -1.28
##  9 Biscoe          -1.11         0.729            -2.07     -1.31    fema~ -1.28
## 10 Dream           -0.822       -0.236            -1.64     -1.19    fema~ -1.28
## # ... with 124 more rows, and 1 more variable: species &lt;fct&gt;</code></pre>
</div>
</div>
<div id="iris-random-forest-model-with-ranger" class="section level2">
<h2>Iris Random Forest Model with <code>ranger</code></h2>
<p>Using the <code>iris</code> dataset and Random Forest Classification.
This part uses <code>rsample</code> to split the data and the <code>recipes</code> to <em>prep</em> the data for model making.</p>
<pre class="r"><code>#set.seed(100)
iris_split &lt;- rsample::initial_split(iris, prop = 0.6)
iris_split</code></pre>
<pre><code>## &lt;Analysis/Assess/Total&gt;
## &lt;90/60/150&gt;</code></pre>
<pre class="r"><code>iris_split %&gt;% training() %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 90
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 6.3, 5.0, 5.3, 4.5, 6.5, 5.0, 5.1, 6.3, 5.6, 7.2, 5.7, 6.~
## $ Sepal.Width  &lt;dbl&gt; 2.5, 3.0, 3.7, 2.3, 3.0, 3.5, 3.8, 2.3, 2.5, 3.2, 4.4, 2.~
## $ Petal.Length &lt;dbl&gt; 4.9, 1.6, 1.5, 1.3, 5.8, 1.3, 1.9, 4.4, 3.9, 6.0, 1.5, 5.~
## $ Petal.Width  &lt;dbl&gt; 1.5, 0.2, 0.2, 0.3, 2.2, 0.3, 0.4, 1.3, 1.1, 1.8, 0.4, 1.~
## $ Species      &lt;fct&gt; versicolor, setosa, setosa, setosa, virginica, setosa, se~</code></pre>
<pre class="r"><code>iris_split %&gt;% testing() %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 4.9, 4.6, 5.0, 4.6, 5.0, 5.8, 5.1, 5.4, 4.8, 5.5, 4.9, 4.~
## $ Sepal.Width  &lt;dbl&gt; 3.0, 3.1, 3.6, 3.4, 3.4, 4.0, 3.8, 3.4, 3.4, 4.2, 3.1, 3.~
## $ Petal.Length &lt;dbl&gt; 1.4, 1.5, 1.4, 1.4, 1.5, 1.2, 1.5, 1.7, 1.9, 1.4, 1.5, 1.~
## $ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.2, 0.3, 0.2, 0.2, 0.3, 0.2, 0.2, 0.2, 0.2, 0.~
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~</code></pre>
<div id="iris-data-pre-processing-creating-the-recipe" class="section level3">
<h3>Iris Data Pre-Processing: Creating the Recipe</h3>
<p>The <code>recipes</code> package provides an interface that specializes in <em>data pre-processing</em>. Within the package, the functions that start, or execute, the data transformations are named after <strong>cooking actions</strong>. That makes the interface more user-friendly. For example:</p>
<ul>
<li><p><code>recipe()</code> - Starts a new set of transformations to be applied, similar to the <code>ggplot()</code> command. Its main argument is the model’s <code>formula</code>.</p></li>
<li><p><code>prep()</code> - Executes the transformations on top of the data that is supplied (<strong>typically, the training data</strong>). Each data transformation is a <code>step()</code> function. ( Recall what we did with the <code>caret</code> package: <em>Centering, Scaling, Removing Correlated variables</em>…)</p></li>
</ul>
<p>Note that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the <strong>train_tbl</strong> only. <a href="https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c" class="uri">https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c</a>
CRAN: The idea is that the preprocessing operations will all be <strong>created</strong> using the <em>training set</em> and then these steps will be <strong>applied</strong> to both the training and test set.</p>
<pre class="r"><code># Pre Processing the Training Data

iris_recipe &lt;- 
  training(iris_split) %&gt;% # Note: Using TRAINING data !!
  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!

# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).</code></pre>
<p>Q: How does the recipe “figure” out which are the outcomes and which are the predictors?
A.The <code>recipe</code> command defines <code>Outcomes</code> and <code>Predictors</code> using the formula interface. <del>Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…</del></p>
<p>Q. Why is the recipe not agnostic to data set? Is that a meaningful question?
A. The use of the <code>training set</code> in the recipe command is just to declare the variables and specify the <code>roles</code> of the data, nothing else. <code>Roles</code> are open-ended and extensible.
From <a href="https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html" class="uri">https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html</a> :</p>
<blockquote>
<p>This document demonstrates some basic uses of recipes. First, some definitions are required:
- <strong>variables</strong> are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y.
- <strong>roles</strong> define how variables will be used in the model. Examples are: <code>predictor</code> (independent variables), <code>response</code>, and <code>case weight</code>. This is meant to be open-ended and extensible.
- <strong>terms</strong> are columns in a <strong>design matrix</strong> such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of <code>principal components</code> or a set of columns, that define a <code>basis function</code> for a variable. These are synonymous with <code>features</code> in machine learning. Variables that have <code>predictor</code> roles would automatically be main <code>effect terms</code>.</p>
</blockquote>
<pre class="r"><code># Apply the transformation steps
iris_recipe &lt;- iris_recipe %&gt;% 
  step_corr(all_predictors()) %&gt;% 
  step_center(all_predictors(), -all_outcomes()) %&gt;% 
  step_scale(all_predictors(), -all_outcomes()) %&gt;% 
  prep()</code></pre>
<p>This has created the <code>recipe()</code> and prepped it too.
We now need to apply it to our datasets:</p>
<ul>
<li>Take <code>training</code> data and <code>bake()</code> it to prepare it for modelling.</li>
<li>Do the same for the <code>testing</code> set.</li>
</ul>
<pre class="r"><code>iris_training_baked &lt;- 
  iris_split %&gt;% 
  training() %&gt;% 
  bake(iris_recipe,.)
iris_training_baked</code></pre>
<pre><code>## # A tibble: 90 x 4
##    Sepal.Length Sepal.Width Petal.Width Species   
##           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     
##  1        0.603      -1.32       0.448  versicolor
##  2       -0.983      -0.148     -1.25   setosa    
##  3       -0.617       1.49      -1.25   setosa    
##  4       -1.59       -1.79      -1.12   setosa    
##  5        0.847      -0.148      1.36   virginica 
##  6       -0.983       1.02      -1.12   setosa    
##  7       -0.861       1.72      -0.986  setosa    
##  8        0.603      -1.79       0.187  versicolor
##  9       -0.251      -1.32      -0.0739 versicolor
## 10        1.70        0.320      0.839  virginica 
## # ... with 80 more rows</code></pre>
<pre class="r"><code>iris_testing_baked &lt;- 
  iris_split %&gt;% 
  testing() %&gt;% 
  bake(iris_recipe,.)
iris_testing_baked </code></pre>
<pre><code>## # A tibble: 60 x 4
##    Sepal.Length Sepal.Width Petal.Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  
##  1     -1.10        -0.148        -1.25 setosa 
##  2     -1.47         0.0858       -1.25 setosa 
##  3     -0.983        1.26         -1.25 setosa 
##  4     -1.47         0.788        -1.12 setosa 
##  5     -0.983        0.788        -1.25 setosa 
##  6     -0.00678      2.19         -1.25 setosa 
##  7     -0.861        1.72         -1.12 setosa 
##  8     -0.495        0.788        -1.25 setosa 
##  9     -1.23         0.788        -1.25 setosa 
## 10     -0.373        2.66         -1.25 setosa 
## # ... with 50 more rows</code></pre>
</div>
<div id="iris-model-training-using-parsnip" class="section level3">
<h3>Iris Model Training using <code>parsnip</code></h3>
<p>Different ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The <code>tidymodels</code> package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.</p>
<p>The <code>parsnip</code> package is a successor to <code>caret</code>.</p>
<p>To model with <code>parsnip</code>:
1. Pick a <code>model</code> :
2. Set the <code>engine</code>
3. Set the <code>mode</code> (if needed): <em>Classification</em> or <em>Regression</em></p>
<p>Check <a href="https://tidymodels.github.io/parsnip/articles/articles/Models.html">here</a> for models available in <code>parsnip</code>.</p>
<ul>
<li><p>Mode: <em>classification</em> and <em>regression</em> in <code>parsnip</code>, each using a variety of models. ( <strong>Which Way</strong>). This defines the form of the output.</p></li>
<li><p>Engine: The <code>engine</code> is the <strong>R package</strong> that is invoked by <code>parsnip</code> to execute the model. E.g <code>glm</code>, <code>glmnet</code>,<code>keras</code>.( <strong>How</strong> ) <code>parsnip</code> provides <strong>wrappers</strong> for models from these packages.</p></li>
<li><p>Model: is the <strong>specific technique</strong> used for the modelling task. E.g <code>linear_reg()</code>, <code>logistic_reg()</code>, <code>mars</code>, <code>decision_tree</code>, <code>nearest_neighbour</code>…(What model).</p></li>
</ul>
<p>and models have:
- <code>hyperparameters</code>: that are numerical or factor variables that <code>tune</code> the model ( Like the alpha beta parameters for Bayesian priors)</p>
<p>We can use the <code>random forest</code> model to <strong>classify</strong> the iris into species. Here Species is the <code>Outcome</code> variable and the rest are <code>predictor</code> variables. The <code>random forest</code> model is provided by the <code>ranger</code> package, to which <code>tidymodels/parsnip</code> provides a simple and consistent interface.</p>
<pre class="r"><code>library(ranger)
iris_ranger &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;ranger&quot;) %&gt;% 
  fit(Species ~ ., data = iris_training_baked)</code></pre>
<p><code>ranger</code> can generate random forest models for <code>classification</code>, <code>regression</code>, <code>survival</code>( time series, time to event stuff). <code>Extreme Forests</code> are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with <code>feature bagging</code>.
We can also run the same model using the <code>randomForest</code> package:</p>
<pre class="r"><code>library(randomForest,quietly = TRUE)</code></pre>
<pre><code>## randomForest 4.6-14</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ranger&#39;:
## 
##     importance</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre class="r"><code>iris_rf &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;randomForest&quot;) %&gt;% 
  fit(Species ~ ., data = iris_training_baked)</code></pre>
</div>
<div id="iris-predictions" class="section level3">
<h3>Iris Predictions</h3>
<p>The <code>predict()</code> function run against a <code>parsnip</code> model returns a prediction <code>tibble</code>. By default, the prediction variable is called <code>.pred_class</code>.</p>
<pre class="r"><code>predict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%  
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~
## $ Sepal.Length &lt;dbl&gt; -1.104487190, -1.470390921, -0.982519279, -1.470390921, -~
## $ Sepal.Width  &lt;dbl&gt; -0.14822246, 0.08581301, 1.25599036, 0.78791942, 0.787919~
## $ Petal.Width  &lt;dbl&gt; -1.2472447, -1.2472447, -1.2472447, -1.1168707, -1.247244~
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, s~</code></pre>
</div>
<div id="iris-classification-model-validation" class="section level3">
<h3>Iris Classification Model Validation</h3>
<p>We use <code>metrics()</code> function from the <code>yardstick</code> package to evaluate how good the model is.</p>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked) %&gt;%
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  yardstick::metrics(truth = Species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.967
## 2 kap      multiclass     0.950</code></pre>
<p>We can also check the metrics for <code>randomForest</code> model:</p>
<pre class="r"><code>predict(iris_rf, iris_testing_baked) %&gt;%
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  yardstick::metrics(truth = Species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 x 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.967
## 2 kap      multiclass     0.950</code></pre>
</div>
<div id="iris-per-classifier-metrics" class="section level3">
<h3>Iris Per-Classifier Metrics</h3>
<p>We can use the parameter <code>type = "prob"</code> in the <code>predict()</code> function to obtain a probability score on each prediction.
<strong>TBD: How is this prob calculated?</strong> Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for <em>all possible outcomes</em> for <strong>each test datum</strong>)</p>
<pre class="r"><code>iris_ranger_probs &lt;- 
  predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(iris_testing_baked)
glimpse(iris_ranger_probs)</code></pre>
<pre><code>## Rows: 60
## Columns: 7
## $ .pred_setosa     &lt;dbl&gt; 0.904828844, 0.970988095, 0.997777778, 0.984583333, 0~
## $ .pred_versicolor &lt;dbl&gt; 0.071190297, 0.020750000, 0.001111111, 0.010000000, 0~
## $ .pred_virginica  &lt;dbl&gt; 0.023980859, 0.008261905, 0.001111111, 0.005416667, 0~
## $ Sepal.Length     &lt;dbl&gt; -1.104487190, -1.470390921, -0.982519279, -1.47039092~
## $ Sepal.Width      &lt;dbl&gt; -0.14822246, 0.08581301, 1.25599036, 0.78791942, 0.78~
## $ Petal.Width      &lt;dbl&gt; -1.2472447, -1.2472447, -1.2472447, -1.1168707, -1.24~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>iris_rf_probs &lt;- 
  predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(iris_testing_baked)
glimpse(iris_rf_probs)</code></pre>
<pre><code>## Rows: 60
## Columns: 7
## $ .pred_setosa     &lt;dbl&gt; 0.94, 0.98, 1.00, 1.00, 1.00, 0.92, 1.00, 0.97, 1.00,~
## $ .pred_versicolor &lt;dbl&gt; 0.02, 0.00, 0.00, 0.00, 0.00, 0.07, 0.00, 0.03, 0.00,~
## $ .pred_virginica  &lt;dbl&gt; 0.04, 0.02, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00,~
## $ Sepal.Length     &lt;dbl&gt; -1.104487190, -1.470390921, -0.982519279, -1.47039092~
## $ Sepal.Width      &lt;dbl&gt; -0.14822246, 0.08581301, 1.25599036, 0.78791942, 0.78~
## $ Petal.Width      &lt;dbl&gt; -1.2472447, -1.2472447, -1.2472447, -1.1168707, -1.24~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code># Tabulating the probabilities
ftable(iris_rf_probs$.pred_versicolor)</code></pre>
<pre><code>##   0 0.01 0.02 0.03 0.05 0.06 0.07 0.08 0.12 0.14 0.17 0.27 0.63 0.66 0.69 0.77 0.83 0.85 0.87 0.88 0.89 0.9 0.91 0.94 0.95 0.97 0.99
##                                                                                                                                     
##  13    5    3    1    1    7    2    1    1    2    2    1    1    1    1    1    1    2    1    2    2   1    2    1    2    2    1</code></pre>
<pre class="r"><code>ftable(iris_rf_probs$.pred_virginica)</code></pre>
<pre><code>##   0 0.01 0.02 0.03 0.04 0.05 0.06 0.07 0.11 0.12 0.13 0.15 0.16 0.19 0.32 0.35 0.73 0.77 0.79 0.83 0.86 0.92 0.93 0.94 0.95 0.96 0.97 0.98 0.99  1
##                                                                                                                                                   
##  11    3    2    2    3    5    2    1    1    1    1    2    1    1    1    1    1    3    1    1    3    1    1    4    1    1    1    1    2  1</code></pre>
<pre class="r"><code>ftable(iris_rf_probs$.pred_setosa)</code></pre>
<pre><code>##   0 0.01 0.02 0.03 0.04 0.07 0.1 0.12 0.17 0.92 0.94 0.97 0.98  1
##                                                                  
##  24    2    5    1    3    3   1    1    3    1    2    1    3 10</code></pre>
<pre><code>
### Iris Classifier: Gain and ROC Curves

We can plot gain and ROC curves for each of these models


```r
iris_ranger_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 134
## Columns: 5
## $ .level          &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;set~
## $ .n              &lt;dbl&gt; 0, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 20, 21, ~
## $ .n_events       &lt;dbl&gt; 0, 4, 5, 6, 8, 9, 11, 12, 13, 14, 15, 16, 17, 17, 17, ~
## $ .percent_tested &lt;dbl&gt; 0.000000, 6.666667, 8.333333, 10.000000, 13.333333, 15~
## $ .percent_found  &lt;dbl&gt; 0.00000, 23.52941, 29.41176, 35.29412, 47.05882, 52.94~</code></pre>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60ranger%60-1.png" width="672" /></p>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 137
## Columns: 4
## $ .level      &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;~
## $ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001000000, 0.001250000, 0.002941176, ~
## $ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.2558140, 0.3255814, 0.3488372, 0.4~
## $ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0~</code></pre>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60ranger%60-2.png" width="672" /></p>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 74
## Columns: 5
## $ .level          &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;set~
## $ .n              &lt;dbl&gt; 0, 10, 13, 14, 16, 17, 20, 21, 22, 25, 28, 29, 34, 36,~
## $ .n_events       &lt;dbl&gt; 0, 10, 13, 14, 16, 17, 17, 17, 17, 17, 17, 17, 17, 17,~
## $ .percent_tested &lt;dbl&gt; 0.000000, 16.666667, 21.666667, 23.333333, 26.666667, ~
## $ .percent_found  &lt;dbl&gt; 0.000000, 58.823529, 76.470588, 82.352941, 94.117647, ~</code></pre>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60randomForest%60-1.png" width="672" /></p>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 77
## Columns: 4
## $ .level      &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;~
## $ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.02, 0.03, 0.04, 0.07, 0.10, 0.12, 0.17~
## $ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.5581395, 0.6046512, 0.7209302, 0.7~
## $ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0~</code></pre>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60randomForest%60-2.png" width="672" /></p>
</div>
<div id="iris-classifier-metrics" class="section level3">
<h3>Iris Classifier: Metrics</h3>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_setosa     &lt;dbl&gt; 0.904828844, 0.970988095, 0.997777778, 0.984583333, 0~
## $ .pred_versicolor &lt;dbl&gt; 0.071190297, 0.020750000, 0.001111111, 0.010000000, 0~
## $ .pred_virginica  &lt;dbl&gt; 0.023980859, 0.008261905, 0.001111111, 0.005416667, 0~
## $ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy    multiclass     0.967
## 2 kap         multiclass     0.950
## 3 mn_log_loss multiclass     0.217
## 4 roc_auc     hand_till      0.981</code></pre>
<pre class="r"><code># And for the `randomForest`method

predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_setosa     &lt;dbl&gt; 0.94, 0.98, 1.00, 1.00, 1.00, 0.92, 1.00, 0.97, 1.00,~
## $ .pred_versicolor &lt;dbl&gt; 0.02, 0.00, 0.00, 0.00, 0.00, 0.07, 0.00, 0.03, 0.00,~
## $ .pred_virginica  &lt;dbl&gt; 0.04, 0.02, 0.00, 0.00, 0.00, 0.01, 0.00, 0.00, 0.00,~
## $ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setos~</code></pre>
<pre class="r"><code>predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)</code></pre>
<pre><code>## # A tibble: 4 x 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy    multiclass     0.967
## 2 kap         multiclass     0.950
## 3 mn_log_loss multiclass     0.184
## 4 roc_auc     hand_till      0.983</code></pre>
</div>
</div>
