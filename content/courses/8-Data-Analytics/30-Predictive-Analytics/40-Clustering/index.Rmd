---
title: ML - Clustering
subtitle: 
date: "`r Sys.Date()`"
external_link: ""
image:
  caption: Photo by <a href="https://unsplash.com/@maddibazzocco?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Maddi Bazzocco</a> on <a href="https://unsplash.com/s/photos/groceries?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
  focal_point: Smart
weight: 40
links:
# - icon: twitter
#   icon_pack: fab
#   name: Follow
#   url: https://twitter.com/arvind_v
slides:
summary: We will look at the basic models for Clustering of Data.
tags:
- Machine Learning
- Orange
- kNN Algorithm
- k Means Algorithm
# url_code: "code/course-related/example/example.html"
# url_pdf: ""
# url_slides: "slides/new/index.html"
# url_video: ""
---

## Introduction

Quoting from <http://baoqiang.org/?p=579>

### k-Nearest-Neighbour and K-Means clustering
These two are arguably the two commonly used cluster methods. One of the reasons is that they are easy to use and also somehow straightforward. 
So how do they work?  

**k-Nearest-Neighbour**:
Provide N n-dimension entries with known associated classes for each 
entry, the number of classes is k, that is, 
$$
\{\vec{x_i}, y_i\} ,\ \vec{x_i} \in\ {\Re^{n}}\ , y_i\ = \{c_1,...c_k\}, 
i = 1...N
$$

For a new entry $\vec{v_j}$, to which class should it belong? We need use
a distance measure to get the k closest entries of the new entry 
\vec{v_{j}}, the final decision is *simple majority vote* based the 
closest k neighbors. The distance metric could be euclidean or other 
similar ones.  

<iframe width="100%" height="735" frameborder="0"
  src="https://observablehq.com/embed/16bc2b3dcb13d1cd@289?cells=viewof+numTrain%2Cviewof+k%2CPlot"></iframe>
  

**K-means**:
Given N n-dimension entries and classify them in k classes. At first, we *randomly* choose k entries and assign them to k clusters. They are the 
seed classes. Then we calculate the distance between each entry and each class. Each entry will be assigned into one class in terms of the its distance to each class, i.e., assign the entry to its closest class. 
After the assignment is complete, we then calculate the centroid of each class based on their new members. After the centroid calculation, we go
back to the distance calculation and therefore new round classification.
We stop the iteration when there is convergence,i.e,, no new centroid 
and classification.

The two methods are all *semi-supervised learning algorithms* because they
do need we provide the number of clusters prior the clustering.

<iframe width="100%" height="853" frameborder="0"
  src="https://observablehq.com/embed/ab4e983a61997013?cells=viewof+seed%2Cviewof+spread%2Cviewof+num_centroids%2Cviewof+selection%2Cviewof+stepslider"></iframe>
  

## Workflow using Orange




## Workflow using Radiant



## Workflow using R


## Conclusion


## References

1. K-means Cluster Analysis. [UC Business Analytics R Programming Guide](https://uc-r.github.io/) <https://uc-r.github.io/kmeans_clustering#optimal>

1. Thean C Lim. Clustering: k-means, k-means ++ and gganimate. <https://theanlim.rbind.io/post/clustering-k-means-k-means-and-gganimate/>
