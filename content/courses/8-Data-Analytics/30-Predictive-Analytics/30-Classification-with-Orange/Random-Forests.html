---
title: "Random Forests"
author: "Arvind Venkatadri"
date: "13/06/2020"
output:
  html_document:
    df_print: paged
    toc: yes
    code_download: TRUE
editor_options: 
  chunk_output_type: inline
---



<div id="references" class="section level2">
<h2>References:</h2>
<ol style="list-style-type: decimal">
<li>Machine Learning Basics - Random Forest at <a href="https://shirinsplayground.netlify.app/2018/10/ml_basics_rf/">Shirin’s Playground</a></li>
<li></li>
</ol>
</div>
<div id="penguin-random-forest-model-withrandomforest" class="section level2">
<h2>Penguin Random Forest Model with<code>randomForest</code></h2>
<p>Using the <code>penguins</code> dataset and Random Forest Classification.</p>
<pre class="r"><code>penguins</code></pre>
<pre><code>## # A tibble: 344 × 8
##    species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year
##    &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;
##  1 Adelie  Torgersen           39.1          18.7               181        3750 male    2007
##  2 Adelie  Torgersen           39.5          17.4               186        3800 female  2007
##  3 Adelie  Torgersen           40.3          18                 195        3250 female  2007
##  4 Adelie  Torgersen           NA            NA                  NA          NA &lt;NA&gt;    2007
##  5 Adelie  Torgersen           36.7          19.3               193        3450 female  2007
##  6 Adelie  Torgersen           39.3          20.6               190        3650 male    2007
##  7 Adelie  Torgersen           38.9          17.8               181        3625 female  2007
##  8 Adelie  Torgersen           39.2          19.6               195        4675 male    2007
##  9 Adelie  Torgersen           34.1          18.1               193        3475 &lt;NA&gt;    2007
## 10 Adelie  Torgersen           42            20.2               190        4250 &lt;NA&gt;    2007
## # … with 334 more rows</code></pre>
<pre class="r"><code>summary(penguins)</code></pre>
<pre><code>##       species          island    bill_length_mm  bill_depth_mm   flipper_length_mm  body_mass_g       sex           year     
##  Adelie   :152   Biscoe   :168   Min.   :32.10   Min.   :13.10   Min.   :172.0     Min.   :2700   female:165   Min.   :2007  
##  Chinstrap: 68   Dream    :124   1st Qu.:39.23   1st Qu.:15.60   1st Qu.:190.0     1st Qu.:3550   male  :168   1st Qu.:2007  
##  Gentoo   :124   Torgersen: 52   Median :44.45   Median :17.30   Median :197.0     Median :4050   NA&#39;s  : 11   Median :2008  
##                                  Mean   :43.92   Mean   :17.15   Mean   :200.9     Mean   :4202                Mean   :2008  
##                                  3rd Qu.:48.50   3rd Qu.:18.70   3rd Qu.:213.0     3rd Qu.:4750                3rd Qu.:2009  
##                                  Max.   :59.60   Max.   :21.50   Max.   :231.0     Max.   :6300                Max.   :2009  
##                                  NA&#39;s   :2       NA&#39;s   :2       NA&#39;s   :2         NA&#39;s   :2</code></pre>
<pre class="r"><code>penguins %&gt;% skimr::skim()</code></pre>
<table>
<caption><span id="tab:penguins">Table 1: </span>Data summary</caption>
<tbody>
<tr class="odd">
<td align="left">Name</td>
<td align="left">Piped data</td>
</tr>
<tr class="even">
<td align="left">Number of rows</td>
<td align="left">344</td>
</tr>
<tr class="odd">
<td align="left">Number of columns</td>
<td align="left">8</td>
</tr>
<tr class="even">
<td align="left">_______________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Column type frequency:</td>
<td align="left"></td>
</tr>
<tr class="even">
<td align="left">factor</td>
<td align="left">3</td>
</tr>
<tr class="odd">
<td align="left">numeric</td>
<td align="left">5</td>
</tr>
<tr class="even">
<td align="left">________________________</td>
<td align="left"></td>
</tr>
<tr class="odd">
<td align="left">Group variables</td>
<td align="left">None</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: factor</strong></p>
<table>
<colgroup>
<col width="16%" />
<col width="12%" />
<col width="16%" />
<col width="9%" />
<col width="10%" />
<col width="33%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="left">ordered</th>
<th align="right">n_unique</th>
<th align="left">top_counts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">species</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Ade: 152, Gen: 124, Chi: 68</td>
</tr>
<tr class="even">
<td align="left">island</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="left">FALSE</td>
<td align="right">3</td>
<td align="left">Bis: 168, Dre: 124, Tor: 52</td>
</tr>
<tr class="odd">
<td align="left">sex</td>
<td align="right">11</td>
<td align="right">0.97</td>
<td align="left">FALSE</td>
<td align="right">2</td>
<td align="left">mal: 168, fem: 165</td>
</tr>
</tbody>
</table>
<p><strong>Variable type: numeric</strong></p>
<table style="width:100%;">
<colgroup>
<col width="17%" />
<col width="9%" />
<col width="13%" />
<col width="7%" />
<col width="6%" />
<col width="6%" />
<col width="7%" />
<col width="7%" />
<col width="6%" />
<col width="6%" />
<col width="5%" />
</colgroup>
<thead>
<tr class="header">
<th align="left">skim_variable</th>
<th align="right">n_missing</th>
<th align="right">complete_rate</th>
<th align="right">mean</th>
<th align="right">sd</th>
<th align="right">p0</th>
<th align="right">p25</th>
<th align="right">p50</th>
<th align="right">p75</th>
<th align="right">p100</th>
<th align="left">hist</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">bill_length_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">43.92</td>
<td align="right">5.46</td>
<td align="right">32.1</td>
<td align="right">39.23</td>
<td align="right">44.45</td>
<td align="right">48.5</td>
<td align="right">59.6</td>
<td align="left">▃▇▇▆▁</td>
</tr>
<tr class="even">
<td align="left">bill_depth_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">17.15</td>
<td align="right">1.97</td>
<td align="right">13.1</td>
<td align="right">15.60</td>
<td align="right">17.30</td>
<td align="right">18.7</td>
<td align="right">21.5</td>
<td align="left">▅▅▇▇▂</td>
</tr>
<tr class="odd">
<td align="left">flipper_length_mm</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">200.92</td>
<td align="right">14.06</td>
<td align="right">172.0</td>
<td align="right">190.00</td>
<td align="right">197.00</td>
<td align="right">213.0</td>
<td align="right">231.0</td>
<td align="left">▂▇▃▅▂</td>
</tr>
<tr class="even">
<td align="left">body_mass_g</td>
<td align="right">2</td>
<td align="right">0.99</td>
<td align="right">4201.75</td>
<td align="right">801.95</td>
<td align="right">2700.0</td>
<td align="right">3550.00</td>
<td align="right">4050.00</td>
<td align="right">4750.0</td>
<td align="right">6300.0</td>
<td align="left">▃▇▆▃▂</td>
</tr>
<tr class="odd">
<td align="left">year</td>
<td align="right">0</td>
<td align="right">1.00</td>
<td align="right">2008.03</td>
<td align="right">0.82</td>
<td align="right">2007.0</td>
<td align="right">2007.00</td>
<td align="right">2008.00</td>
<td align="right">2009.0</td>
<td align="right">2009.0</td>
<td align="left">▇▁▇▁▇</td>
</tr>
</tbody>
</table>
<pre class="r"><code>penguins &lt;- penguins %&gt;% tidyr::drop_na()
# Spent one hour trying to find `drop-na()` ( 14 June 2020)</code></pre>
<pre class="r"><code># library(corrplot)
cor &lt;- penguins %&gt;% select(is.numeric) %&gt;% cor() </code></pre>
<pre><code>## Warning: Predicate functions must be wrapped in `where()`.
## 
##   # Bad
##   data %&gt;% select(is.numeric)
## 
##   # Good
##   data %&gt;% select(where(is.numeric))
## 
## ℹ Please update your code.
## This message is displayed once per session.</code></pre>
<pre class="r"><code>cor %&gt;% corrplot(., method = &quot;ellipse&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/EDA%20on%20penguins%20data-1.png" width="672" /></p>
<pre class="r"><code># try these too:
# cor %&gt;% corrplot(., method = &quot;square&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)
# cor %&gt;% corrplot(., method = &quot;color&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)
# cor %&gt;% corrplot(., method = &quot;shade&quot;, order = &quot;hclust&quot;,tl.cex = 0.5)</code></pre>
<p>Notes:
- <code>flipper_length_mm</code> and <code>culmen_depth_mm</code> are negtively correlated at approx (-0.7)
- <code>flipper_length_mm</code> and <code>body_mass_g</code> are positively correlated at approx 0.8</p>
<p>So we will use steps in the recipe to remove correlated variables.</p>
<div id="penguin-data-sampling-and-recipe" class="section level3">
<h3>Penguin Data Sampling and Recipe</h3>
<pre class="r"><code># Data Split
penguin_split &lt;- initial_split(penguins, prop = 0.6)
penguin_train &lt;- training(penguin_split)
penguin_test &lt;- testing(penguin_split)
penguin_split</code></pre>
<pre><code>## &lt;Training/Testing/Total&gt;
## &lt;199/134/333&gt;</code></pre>
<pre class="r"><code>head(penguin_train)</code></pre>
<pre><code>## # A tibble: 6 × 8
##   species   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year
##   &lt;fct&gt;     &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;
## 1 Adelie    Biscoe              37.7          18.7               180        3600 male    2007
## 2 Adelie    Biscoe              37.8          20                 190        4250 male    2009
## 3 Adelie    Dream               39.5          16.7               178        3250 female  2007
## 4 Adelie    Torgersen           39.2          19.6               195        4675 male    2007
## 5 Adelie    Dream               36.3          19.5               190        3800 male    2008
## 6 Chinstrap Dream               49.6          18.2               193        3775 male    2009</code></pre>
<pre class="r"><code># Recipe
penguin_recipe &lt;- penguins %&gt;% 
  recipe(species ~ .) %&gt;% 
  step_normalize(all_numeric()) %&gt;% # Scaling and Centering
  step_corr(all_numeric()) %&gt;%  # Handling correlated variables
  prep()

# Baking the data
penguin_train_baked &lt;-  penguin_train %&gt;% 
  bake(object = penguin_recipe, new_data = .)

penguin_test_baked &lt;-  penguin_test %&gt;% 
  bake(object = penguin_recipe, new_data = .)

head(penguin_train_baked)</code></pre>
<pre><code>## # A tibble: 6 × 8
##   island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex       year species  
##   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;fct&gt;    
## 1 Biscoe            -1.15          0.780            -1.50      -0.754  male   -1.28   Adelie   
## 2 Biscoe            -1.13          1.44             -0.782      0.0533 male    1.18   Adelie   
## 3 Dream             -0.822        -0.236            -1.64      -1.19   female -1.28   Adelie   
## 4 Torgersen         -0.876         1.24             -0.426      0.581  male   -1.28   Adelie   
## 5 Dream             -1.41          1.19             -0.782     -0.506  male   -0.0517 Adelie   
## 6 Dream              1.03          0.526            -0.568     -0.537  male    1.18   Chinstrap</code></pre>
</div>
<div id="penguin-random-forest-model" class="section level3">
<h3>Penguin Random Forest Model</h3>
<pre class="r"><code>penguin_model &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_engine(&quot;randomForest&quot;) %&gt;% 
  set_mode(&quot;classification&quot;)
penguin_model</code></pre>
<pre><code>## Random Forest Model Specification (classification)
## 
## Main Arguments:
##   trees = 100
## 
## Computational engine: randomForest</code></pre>
<pre class="r"><code>penguin_fit &lt;- 
  penguin_model %&gt;% 
  fit(species ~ .,penguin_train_baked)
penguin_fit</code></pre>
<pre><code>## parsnip model object
## 
## 
## Call:
##  randomForest(x = maybe_data_frame(x), y = y, ntree = ~100) 
##                Type of random forest: classification
##                      Number of trees: 100
## No. of variables tried at each split: 2
## 
##         OOB estimate of  error rate: 0.5%
## Confusion matrix:
##           Adelie Chinstrap Gentoo class.error
## Adelie        87         1      0  0.01136364
## Chinstrap      0        40      0  0.00000000
## Gentoo         0         0     71  0.00000000</code></pre>
<pre class="r"><code># iris_ranger &lt;- 
#   rand_forest(trees = 100) %&gt;% 
#   set_mode(&quot;classification&quot;) %&gt;% 
#   set_engine(&quot;ranger&quot;) %&gt;% 
#   fit(Species ~ ., data = iris_training_baked)</code></pre>
</div>
<div id="metrics-for-the-penguin-random-forest-model" class="section level3">
<h3>Metrics for the Penguin Random Forest Model</h3>
<pre class="r"><code># Predictions
predict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  
  dplyr::bind_cols(penguin_test_baked) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 134
## Columns: 9
## $ .pred_class       &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie…
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, Biscoe, Biscoe, Dream, Dream…
## $ bill_length_mm    &lt;dbl&gt; -0.8215515, -0.6752636, -1.3335592, -0.9312674, -0.9861254, -1.3518452, -1.0592694, -0.6386916, -0.638…
## $ bill_depth_mm     &lt;dbl&gt; 0.11940428, 0.42409105, 1.08424573, 0.32252879, 2.04908718, 0.32252879, 0.47487218, 0.37330992, 0.8811…
## $ flipper_length_mm &lt;dbl&gt; -1.06786655, -0.42573251, -0.56842897, -1.42460769, -0.71112542, -1.13921478, -1.13921478, -0.99651833…
## $ body_mass_g       &lt;dbl&gt; -0.505525421, -1.188572125, -0.940191505, -0.722858463, -0.505525421, -0.629715731, -0.319239956, -1.2…
## $ sex               &lt;fct&gt; female, female, female, female, male, female, male, female, male, male, female, male, male, male, male…
## $ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.281…
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie…</code></pre>
<pre class="r"><code># Prediction Accuracy Metrics
predict(object = penguin_fit, new_data = penguin_test_baked) %&gt;%  
  dplyr::bind_cols(penguin_test_baked) %&gt;% 
  yardstick::metrics(truth = species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.970
## 2 kap      multiclass     0.954</code></pre>
<pre class="r"><code># Prediction Probabilities
penguin_fit_probs &lt;- 
  predict(penguin_fit, penguin_test_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(penguin_test_baked)
glimpse(penguin_fit_probs)</code></pre>
<pre><code>## Rows: 134
## Columns: 11
## $ .pred_Adelie      &lt;dbl&gt; 0.96, 0.99, 0.99, 0.98, 0.99, 0.98, 1.00, 1.00, 1.00, 0.96, 0.94, 0.93, 0.88, 0.89, 0.28, 0.93, 0.79, …
## $ .pred_Chinstrap   &lt;dbl&gt; 0.04, 0.01, 0.01, 0.02, 0.01, 0.02, 0.00, 0.00, 0.00, 0.04, 0.06, 0.07, 0.12, 0.11, 0.70, 0.07, 0.20, …
## $ .pred_Gentoo      &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.02, 0.00, 0.01, …
## $ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Torgersen, Biscoe, Biscoe, Biscoe, Dream, Dream…
## $ bill_length_mm    &lt;dbl&gt; -0.8215515, -0.6752636, -1.3335592, -0.9312674, -0.9861254, -1.3518452, -1.0592694, -0.6386916, -0.638…
## $ bill_depth_mm     &lt;dbl&gt; 0.11940428, 0.42409105, 1.08424573, 0.32252879, 2.04908718, 0.32252879, 0.47487218, 0.37330992, 0.8811…
## $ flipper_length_mm &lt;dbl&gt; -1.06786655, -0.42573251, -0.56842897, -1.42460769, -0.71112542, -1.13921478, -1.13921478, -0.99651833…
## $ body_mass_g       &lt;dbl&gt; -0.505525421, -1.188572125, -0.940191505, -0.722858463, -0.505525421, -0.629715731, -0.319239956, -1.2…
## $ sex               &lt;fct&gt; female, female, female, female, male, female, male, female, male, male, female, male, male, male, male…
## $ year              &lt;dbl&gt; -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.2818130, -1.281…
## $ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adelie…</code></pre>
<pre class="r"><code># Confusion Matrix
penguin_fit$fit$confusion %&gt;% tidy()</code></pre>
<pre><code>## Warning: &#39;tidy.numeric&#39; is deprecated.
## See help(&quot;Deprecated&quot;)</code></pre>
<pre><code>## # A tibble: 3 × 1
##   x[,&quot;Adelie&quot;] [,&quot;Chinstrap&quot;] [,&quot;Gentoo&quot;] [,&quot;class.error&quot;]
##          &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;            &lt;dbl&gt;
## 1           87              1           0           0.0114
## 2            0             40           0           0     
## 3            0              0          71           0</code></pre>
<pre class="r"><code># Gain Curves
penguin_fit_probs %&gt;% 
  yardstick::gain_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Penguin%20Model%20Predictions%20and%20Metrics-1.png" width="672" /></p>
<pre class="r"><code># ROC Plot
penguin_fit_probs%&gt;%
  roc_curve(species, .pred_Adelie:.pred_Gentoo) %&gt;%
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Penguin%20Model%20Predictions%20and%20Metrics-2.png" width="672" />
### Using <code>broom</code> on the penguin model</p>
<pre class="r"><code>penguin_split</code></pre>
<pre><code>## &lt;Training/Testing/Total&gt;
## &lt;199/134/333&gt;</code></pre>
<pre class="r"><code>penguin_split %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 333 × 2
##      Row Data    
##    &lt;int&gt; &lt;chr&gt;   
##  1     1 Analysis
##  2     5 Analysis
##  3     7 Analysis
##  4     8 Analysis
##  5    10 Analysis
##  6    12 Analysis
##  7    13 Analysis
##  8    14 Analysis
##  9    15 Analysis
## 10    16 Analysis
## # … with 323 more rows</code></pre>
<pre class="r"><code>penguin_recipe %&gt;% broom::tidy()</code></pre>
<pre><code>## # A tibble: 2 × 6
##   number operation type      trained skip  id             
##    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;lgl&gt;   &lt;lgl&gt; &lt;chr&gt;          
## 1      1 step      normalize TRUE    FALSE normalize_t6Rbj
## 2      2 step      corr      TRUE    FALSE corr_2EOuA</code></pre>
<pre class="r"><code># Following do not work for `random forest models` !! ;-()
#penguin_model %&gt;% tidy()
#penguin_fit %&gt;% tidy() 
penguin_model %&gt;% str()</code></pre>
<pre><code>## List of 5
##  $ args    :List of 3
##   ..$ mtry : language ~NULL
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##   ..$ trees: language ~100
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##   ..$ min_n: language ~NULL
##   .. ..- attr(*, &quot;.Environment&quot;)=&lt;environment: R_EmptyEnv&gt; 
##  $ eng_args: Named list()
##   ..- attr(*, &quot;class&quot;)= chr [1:2] &quot;quosures&quot; &quot;list&quot;
##  $ mode    : chr &quot;classification&quot;
##  $ method  : NULL
##  $ engine  : chr &quot;randomForest&quot;
##  - attr(*, &quot;class&quot;)= chr [1:2] &quot;rand_forest&quot; &quot;model_spec&quot;</code></pre>
<pre class="r"><code>penguin_test_baked</code></pre>
<pre><code>## # A tibble: 134 × 8
##    island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g sex     year species
##    &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  &lt;dbl&gt; &lt;fct&gt;  
##  1 Torgersen         -0.822         0.119            -1.07       -0.506 female -1.28 Adelie 
##  2 Torgersen         -0.675         0.424            -0.426      -1.19  female -1.28 Adelie 
##  3 Torgersen         -1.33          1.08             -0.568      -0.940 female -1.28 Adelie 
##  4 Torgersen         -0.931         0.323            -1.42       -0.723 female -1.28 Adelie 
##  5 Torgersen         -0.986         2.05             -0.711      -0.506 male   -1.28 Adelie 
##  6 Torgersen         -1.35          0.323            -1.14       -0.630 female -1.28 Adelie 
##  7 Biscoe            -1.06          0.475            -1.14       -0.319 male   -1.28 Adelie 
##  8 Biscoe            -0.639         0.373            -0.997      -1.25  female -1.28 Adelie 
##  9 Biscoe            -0.639         0.881            -1.50       -0.319 male   -1.28 Adelie 
## 10 Dream             -1.24          0.475            -1.64       -0.381 male   -1.28 Adelie 
## # … with 124 more rows</code></pre>
</div>
</div>
<div id="iris-random-forest-model-with-ranger" class="section level2">
<h2>Iris Random Forest Model with <code>ranger</code></h2>
<p>Using the <code>iris</code> dataset and Random Forest Classification.
This part uses <code>rsample</code> to split the data and the <code>recipes</code> to <em>prep</em> the data for model making.</p>
<pre class="r"><code>#set.seed(100)
iris_split &lt;- rsample::initial_split(iris, prop = 0.6)
iris_split</code></pre>
<pre><code>## &lt;Training/Testing/Total&gt;
## &lt;90/60/150&gt;</code></pre>
<pre class="r"><code>iris_split %&gt;% training() %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 90
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 6.9, 5.5, 6.7, 5.1, 7.4, 6.3, 6.7, 5.6, 5.0, 5.7, 6.5, 5.1, 5.7, 5.4, 5.1, 7.3, 5.2, 5.7, 4.7, 6.3, 5.8, 5.…
## $ Sepal.Width  &lt;dbl&gt; 3.1, 2.4, 3.1, 3.8, 2.8, 2.5, 3.1, 2.9, 3.4, 2.8, 3.0, 3.7, 4.4, 3.4, 3.3, 2.9, 4.1, 2.8, 3.2, 2.7, 2.7, 2.…
## $ Petal.Length &lt;dbl&gt; 4.9, 3.8, 4.4, 1.6, 6.1, 4.9, 5.6, 3.6, 1.6, 4.5, 5.2, 1.5, 1.5, 1.5, 1.7, 6.3, 1.5, 4.1, 1.3, 4.9, 3.9, 3.…
## $ Petal.Width  &lt;dbl&gt; 1.5, 1.1, 1.4, 0.2, 1.9, 1.5, 2.4, 1.3, 0.4, 1.3, 2.0, 0.4, 0.4, 0.4, 0.5, 1.8, 0.1, 1.3, 0.2, 1.8, 1.2, 1.…
## $ Species      &lt;fct&gt; versicolor, versicolor, versicolor, setosa, virginica, versicolor, virginica, versicolor, setosa, versicolo…</code></pre>
<pre class="r"><code>iris_split %&gt;% testing() %&gt;% glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ Sepal.Length &lt;dbl&gt; 4.9, 4.6, 5.4, 4.6, 5.0, 4.8, 4.8, 4.3, 5.8, 5.4, 5.7, 4.8, 5.2, 4.8, 5.5, 5.0, 5.5, 4.9, 5.1, 5.0, 4.5, 5.…
## $ Sepal.Width  &lt;dbl&gt; 3.0, 3.1, 3.9, 3.4, 3.4, 3.4, 3.0, 3.0, 4.0, 3.9, 3.8, 3.4, 3.4, 3.1, 4.2, 3.2, 3.5, 3.6, 3.4, 3.5, 2.3, 3.…
## $ Petal.Length &lt;dbl&gt; 1.4, 1.5, 1.7, 1.4, 1.5, 1.6, 1.4, 1.1, 1.2, 1.3, 1.7, 1.9, 1.4, 1.6, 1.4, 1.2, 1.3, 1.4, 1.5, 1.3, 1.3, 1.…
## $ Petal.Width  &lt;dbl&gt; 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.1, 0.1, 0.2, 0.4, 0.3, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.1, 0.2, 0.3, 0.3, 0.…
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, set…</code></pre>
<div id="iris-data-pre-processing-creating-the-recipe" class="section level3">
<h3>Iris Data Pre-Processing: Creating the Recipe</h3>
<p>The <code>recipes</code> package provides an interface that specializes in <em>data pre-processing</em>. Within the package, the functions that start, or execute, the data transformations are named after <strong>cooking actions</strong>. That makes the interface more user-friendly. For example:</p>
<ul>
<li><p><code>recipe()</code> - Starts a new set of transformations to be applied, similar to the <code>ggplot()</code> command. Its main argument is the model’s <code>formula</code>.</p></li>
<li><p><code>prep()</code> - Executes the transformations on top of the data that is supplied (<strong>typically, the training data</strong>). Each data transformation is a <code>step()</code> function. ( Recall what we did with the <code>caret</code> package: <em>Centering, Scaling, Removing Correlated variables</em>…)</p></li>
</ul>
<p>Note that in order to avoid data leakage (e.g: transferring information from the train set into the test set), data should be “prepped” using the <strong>train_tbl</strong> only. <a href="https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c" class="uri">https://towardsdatascience.com/modelling-with-tidymodels-and-parsnip-bae2c01c131c</a>
CRAN: The idea is that the preprocessing operations will all be <strong>created</strong> using the <em>training set</em> and then these steps will be <strong>applied</strong> to both the training and test set.</p>
<pre class="r"><code># Pre Processing the Training Data

iris_recipe &lt;- 
  training(iris_split) %&gt;% # Note: Using TRAINING data !!
  recipe(Species ~ .)      # Note: Outcomes ~ Predictors !!

# The data contained in the `data` argument need not be the training set; this data is only used to catalog the names of the variables and their types (e.g. numeric, etc.).</code></pre>
<p>Q: How does the recipe “figure” out which are the outcomes and which are the predictors?
A.The <code>recipe</code> command defines <code>Outcomes</code> and <code>Predictors</code> using the formula interface. <del>Not clear how this recipe “figures” out which are the outcomes and which are the predictors, when we have not yet specified them…</del></p>
<p>Q. Why is the recipe not agnostic to data set? Is that a meaningful question?
A. The use of the <code>training set</code> in the recipe command is just to declare the variables and specify the <code>roles</code> of the data, nothing else. <code>Roles</code> are open-ended and extensible.
From <a href="https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html" class="uri">https://cran.r-project.org/web/packages/recipes/vignettes/Simple_Example.html</a> :</p>
<blockquote>
<p>This document demonstrates some basic uses of recipes. First, some definitions are required:
- <strong>variables</strong> are the original (raw) data columns in a data frame or tibble. For example, in a traditional formula Y ~ A + B + A:B, the variables are A, B, and Y.
- <strong>roles</strong> define how variables will be used in the model. Examples are: <code>predictor</code> (independent variables), <code>response</code>, and <code>case weight</code>. This is meant to be open-ended and extensible.
- <strong>terms</strong> are columns in a <strong>design matrix</strong> such as A, B, and A:B. These can be other derived entities that are grouped, such as a set of <code>principal components</code> or a set of columns, that define a <code>basis function</code> for a variable. These are synonymous with <code>features</code> in machine learning. Variables that have <code>predictor</code> roles would automatically be main <code>effect terms</code>.</p>
</blockquote>
<pre class="r"><code># Apply the transformation steps
iris_recipe &lt;- iris_recipe %&gt;% 
  step_corr(all_predictors()) %&gt;% 
  step_center(all_predictors(), -all_outcomes()) %&gt;% 
  step_scale(all_predictors(), -all_outcomes()) %&gt;% 
  prep()</code></pre>
<p>This has created the <code>recipe()</code> and prepped it too.
We now need to apply it to our datasets:</p>
<ul>
<li>Take <code>training</code> data and <code>bake()</code> it to prepare it for modelling.</li>
<li>Do the same for the <code>testing</code> set.</li>
</ul>
<pre class="r"><code>iris_training_baked &lt;- 
  iris_split %&gt;% 
  training() %&gt;% 
  bake(iris_recipe,.)
iris_training_baked</code></pre>
<pre><code>## # A tibble: 90 × 4
##    Sepal.Length Sepal.Width Petal.Width Species   
##           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;     
##  1        1.16        0.150      0.311  versicolor
##  2       -0.527      -1.63      -0.233  versicolor
##  3        0.917       0.150      0.175  versicolor
##  4       -1.01        1.93      -1.46   setosa    
##  5        1.76       -0.613      0.855  virginica 
##  6        0.436      -1.38       0.311  versicolor
##  7        0.917       0.150      1.54   virginica 
##  8       -0.406      -0.359      0.0393 versicolor
##  9       -1.13        0.912     -1.18   setosa    
## 10       -0.286      -0.613      0.0393 versicolor
## # … with 80 more rows</code></pre>
<pre class="r"><code>iris_testing_baked &lt;- 
  iris_split %&gt;% 
  testing() %&gt;% 
  bake(iris_recipe,.)
iris_testing_baked </code></pre>
<pre><code>## # A tibble: 60 × 4
##    Sepal.Length Sepal.Width Petal.Width Species
##           &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt; &lt;fct&gt;  
##  1       -1.25       -0.104       -1.46 setosa 
##  2       -1.61        0.150       -1.46 setosa 
##  3       -0.647       2.18        -1.18 setosa 
##  4       -1.61        0.912       -1.32 setosa 
##  5       -1.13        0.912       -1.46 setosa 
##  6       -1.37        0.912       -1.46 setosa 
##  7       -1.37       -0.104       -1.59 setosa 
##  8       -1.97       -0.104       -1.59 setosa 
##  9       -0.166       2.44        -1.46 setosa 
## 10       -0.647       2.18        -1.18 setosa 
## # … with 50 more rows</code></pre>
</div>
<div id="iris-model-training-using-parsnip" class="section level3">
<h3>Iris Model Training using <code>parsnip</code></h3>
<p>Different ML packages provide different interfaces (APIs ) to do the same thing (e.g random forests). The <code>tidymodels</code> package provides a consistent interface to invoke a wide variety of packages supporting a wide variety of models.</p>
<p>The <code>parsnip</code> package is a successor to <code>caret</code>.</p>
<p>To model with <code>parsnip</code>:
1. Pick a <code>model</code> :
2. Set the <code>engine</code>
3. Set the <code>mode</code> (if needed): <em>Classification</em> or <em>Regression</em></p>
<p>Check <a href="https://tidymodels.github.io/parsnip/articles/articles/Models.html">here</a> for models available in <code>parsnip</code>.</p>
<ul>
<li><p>Mode: <em>classification</em> and <em>regression</em> in <code>parsnip</code>, each using a variety of models. ( <strong>Which Way</strong>). This defines the form of the output.</p></li>
<li><p>Engine: The <code>engine</code> is the <strong>R package</strong> that is invoked by <code>parsnip</code> to execute the model. E.g <code>glm</code>, <code>glmnet</code>,<code>keras</code>.( <strong>How</strong> ) <code>parsnip</code> provides <strong>wrappers</strong> for models from these packages.</p></li>
<li><p>Model: is the <strong>specific technique</strong> used for the modelling task. E.g <code>linear_reg()</code>, <code>logistic_reg()</code>, <code>mars</code>, <code>decision_tree</code>, <code>nearest_neighbour</code>…(What model).</p></li>
</ul>
<p>and models have:
- <code>hyperparameters</code>: that are numerical or factor variables that <code>tune</code> the model ( Like the alpha beta parameters for Bayesian priors)</p>
<p>We can use the <code>random forest</code> model to <strong>classify</strong> the iris into species. Here Species is the <code>Outcome</code> variable and the rest are <code>predictor</code> variables. The <code>random forest</code> model is provided by the <code>ranger</code> package, to which <code>tidymodels/parsnip</code> provides a simple and consistent interface.</p>
<pre class="r"><code>library(ranger)
iris_ranger &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;ranger&quot;) %&gt;% 
  fit(Species ~ ., data = iris_training_baked)</code></pre>
<p><code>ranger</code> can generate random forest models for <code>classification</code>, <code>regression</code>, <code>survival</code>( time series, time to event stuff). <code>Extreme Forests</code> are also supported, wherein all points in the dataset are used ( instead of bootstrap samples) along with <code>feature bagging</code>.
We can also run the same model using the <code>randomForest</code> package:</p>
<pre class="r"><code>library(randomForest,quietly = TRUE)</code></pre>
<pre><code>## randomForest 4.7-1.1</code></pre>
<pre><code>## Type rfNews() to see new features/changes/bug fixes.</code></pre>
<pre><code>## 
## Attaching package: &#39;randomForest&#39;</code></pre>
<pre><code>## The following object is masked from &#39;package:ranger&#39;:
## 
##     importance</code></pre>
<pre><code>## The following object is masked from &#39;package:ggplot2&#39;:
## 
##     margin</code></pre>
<pre><code>## The following object is masked from &#39;package:dplyr&#39;:
## 
##     combine</code></pre>
<pre class="r"><code>iris_rf &lt;- 
  rand_forest(trees = 100) %&gt;% 
  set_mode(&quot;classification&quot;) %&gt;% 
  set_engine(&quot;randomForest&quot;) %&gt;% 
  fit(Species ~ ., data = iris_training_baked)</code></pre>
</div>
<div id="iris-predictions" class="section level3">
<h3>Iris Predictions</h3>
<p>The <code>predict()</code> function run against a <code>parsnip</code> model returns a prediction <code>tibble</code>. By default, the prediction variable is called <code>.pred_class</code>.</p>
<pre class="r"><code>predict(object = iris_ranger, new_data = iris_testing_baked) %&gt;%  
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_class  &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, set…
## $ Sepal.Length &lt;dbl&gt; -1.24850886, -1.60942685, -0.64697890, -1.60942685, -1.12820287, -1.36881486, -1.36881486, -1.97034483, -0.…
## $ Sepal.Width  &lt;dbl&gt; -0.1044871, 0.1496708, 2.1829342, 0.9121446, 0.9121446, 0.9121446, -0.1044871, -0.1044871, 2.4370921, 2.182…
## $ Petal.Width  &lt;dbl&gt; -1.45661460, -1.45661460, -1.18463262, -1.32062361, -1.45661460, -1.45661460, -1.59260559, -1.59260559, -1.…
## $ Species      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, set…</code></pre>
</div>
<div id="iris-classification-model-validation" class="section level3">
<h3>Iris Classification Model Validation</h3>
<p>We use <code>metrics()</code> function from the <code>yardstick</code> package to evaluate how good the model is.</p>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked) %&gt;%
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  yardstick::metrics(truth = Species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.967
## 2 kap      multiclass     0.949</code></pre>
<p>We can also check the metrics for <code>randomForest</code> model:</p>
<pre class="r"><code>predict(iris_rf, iris_testing_baked) %&gt;%
  dplyr::bind_cols(iris_testing_baked) %&gt;% 
  yardstick::metrics(truth = Species, estimate = .pred_class)</code></pre>
<pre><code>## # A tibble: 2 × 3
##   .metric  .estimator .estimate
##   &lt;chr&gt;    &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy multiclass     0.967
## 2 kap      multiclass     0.949</code></pre>
</div>
<div id="iris-per-classifier-metrics" class="section level3">
<h3>Iris Per-Classifier Metrics</h3>
<p>We can use the parameter <code>type = "prob"</code> in the <code>predict()</code> function to obtain a probability score on each prediction.
<strong>TBD: How is this prob calculated?</strong> Possible answer: the Random Forest model outputs its answer by majority voting across n trees. Each of the possible answers( i.e. predictions) for a particular test datum gets a share of the vote, that represents its probability. Hence each dataum in the test vector can show a probability for the “winning” answer. ( Quite possibly we can get the probabilities for <em>all possible outcomes</em> for <strong>each test datum</strong>)</p>
<pre class="r"><code>iris_ranger_probs &lt;- 
  predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(iris_testing_baked)
glimpse(iris_ranger_probs)</code></pre>
<pre><code>## Rows: 60
## Columns: 7
## $ .pred_setosa     &lt;dbl&gt; 0.890134921, 0.944750000, 0.953825397, 0.987750000, 0.978861111, 0.982750000, 0.890134921, 0.915134921,…
## $ .pred_versicolor &lt;dbl&gt; 0.102753968, 0.040638889, 0.036285714, 0.008888889, 0.016388889, 0.011388889, 0.102753968, 0.080253968,…
## $ .pred_virginica  &lt;dbl&gt; 0.007111111, 0.014611111, 0.009888889, 0.003361111, 0.004750000, 0.005861111, 0.007111111, 0.004611111,…
## $ Sepal.Length     &lt;dbl&gt; -1.24850886, -1.60942685, -0.64697890, -1.60942685, -1.12820287, -1.36881486, -1.36881486, -1.97034483,…
## $ Sepal.Width      &lt;dbl&gt; -0.1044871, 0.1496708, 2.1829342, 0.9121446, 0.9121446, 0.9121446, -0.1044871, -0.1044871, 2.4370921, 2…
## $ Petal.Width      &lt;dbl&gt; -1.45661460, -1.45661460, -1.18463262, -1.32062361, -1.45661460, -1.45661460, -1.59260559, -1.59260559,…
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa,…</code></pre>
<pre class="r"><code>iris_rf_probs &lt;- 
  predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;%
  dplyr::bind_cols(iris_testing_baked)
glimpse(iris_rf_probs)</code></pre>
<pre><code>## Rows: 60
## Columns: 7
## $ .pred_setosa     &lt;dbl&gt; 0.92, 0.97, 0.99, 1.00, 1.00, 1.00, 0.93, 0.96, 0.74, 0.99, 0.72, 1.00, 1.00, 0.97, 0.84, 0.96, 0.71, 1…
## $ .pred_versicolor &lt;dbl&gt; 0.07, 0.03, 0.01, 0.00, 0.00, 0.00, 0.06, 0.04, 0.25, 0.01, 0.28, 0.00, 0.00, 0.03, 0.16, 0.04, 0.29, 0…
## $ .pred_virginica  &lt;dbl&gt; 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…
## $ Sepal.Length     &lt;dbl&gt; -1.24850886, -1.60942685, -0.64697890, -1.60942685, -1.12820287, -1.36881486, -1.36881486, -1.97034483,…
## $ Sepal.Width      &lt;dbl&gt; -0.1044871, 0.1496708, 2.1829342, 0.9121446, 0.9121446, 0.9121446, -0.1044871, -0.1044871, 2.4370921, 2…
## $ Petal.Width      &lt;dbl&gt; -1.45661460, -1.45661460, -1.18463262, -1.32062361, -1.45661460, -1.45661460, -1.59260559, -1.59260559,…
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa,…</code></pre>
<pre class="r"><code># Tabulating the probabilities
ftable(iris_rf_probs$.pred_versicolor)</code></pre>
<pre><code>##   0 0.01 0.03 0.04 0.05 0.06 0.07 0.08 0.13 0.15 0.16 0.2 0.25 0.28 0.29 0.3 0.33 0.65 0.68 0.69 0.82 0.86 0.88 0.89 0.9 0.91 0.92 0.97 0.98 0.99  1
##                                                                                                                                                     
##  12    2    3    3    2    5    1    1    3    2    1   1    1    2    2   1    1    1    1    1    1    2    1    1   1    1    2    1    1    1  2</code></pre>
<pre class="r"><code>ftable(iris_rf_probs$.pred_virginica)</code></pre>
<pre><code>##   0 0.01 0.02 0.03 0.08 0.09 0.1 0.11 0.12 0.14 0.26 0.31 0.32 0.61 0.64 0.71 0.72 0.84 0.85 0.87 0.93 0.94 0.95 0.96 0.97  1
##                                                                                                                              
##  22    5    2    1    2    2   1    1    2    1    1    1    1    1    1    1    1    1    2    4    2    1    1    1    1  1</code></pre>
<pre class="r"><code>ftable(iris_rf_probs$.pred_setosa)</code></pre>
<pre><code>##   0 0.01 0.02 0.06 0.08 0.09 0.16 0.71 0.72 0.74 0.84 0.92 0.93 0.96 0.97 0.99  1
##                                                                                  
##  26    2    1    2    2    1    1    2    1    1    1    1    2    2    2    2 11</code></pre>
<pre><code>
### Iris Classifier: Gain and ROC Curves

We can plot gain and ROC curves for each of these models


```r
iris_ranger_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 144
## Columns: 5
## $ .level          &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;set…
## $ .n              &lt;dbl&gt; 0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 26, 28, 29, 30, 32, 33, 34, …
## $ .n_events       &lt;dbl&gt; 0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 12, 14, 15, 16, 17, 20, 21, 22, 23, 24, 25, 25, 25, 25, 25, 25, 25, 25, …
## $ .percent_tested &lt;dbl&gt; 0.000000, 3.333333, 5.000000, 6.666667, 8.333333, 11.666667, 13.333333, 15.000000, 16.666667, 18.333333,…
## $ .percent_found  &lt;dbl&gt; 0.000000, 8.000000, 12.000000, 16.000000, 20.000000, 28.000000, 32.000000, 36.000000, 40.000000, 44.0000…</code></pre>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60ranger%60-1.png" width="672" /></p>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 147
## Columns: 4
## $ .level      &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;…
## $ .threshold  &lt;dbl&gt; -Inf, 0.000000000, 0.001250000, 0.003020202, 0.004000000, 0.005250000, 0.007750000, 0.008285714, 0.022278139…
## $ specificity &lt;dbl&gt; 0.00000000, 0.00000000, 0.42857143, 0.48571429, 0.51428571, 0.60000000, 0.62857143, 0.65714286, 0.68571429, …
## $ sensitivity &lt;dbl&gt; 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, 1.00, …</code></pre>
<pre class="r"><code>iris_ranger_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60ranger%60-2.png" width="672" /></p>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 77
## Columns: 5
## $ .level          &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;set…
## $ .n              &lt;dbl&gt; 0, 11, 13, 15, 17, 19, 20, 21, 22, 23, 25, 26, 27, 29, 31, 32, 34, 60, 0, 2, 3, 4, 5, 7, 8, 9, 10, 11, 1…
## $ .n_events       &lt;dbl&gt; 0, 11, 13, 15, 17, 19, 20, 21, 22, 23, 25, 25, 25, 25, 25, 25, 25, 25, 0, 2, 3, 4, 5, 6, 6, 7, 8, 9, 11,…
## $ .percent_tested &lt;dbl&gt; 0.000000, 18.333333, 21.666667, 25.000000, 28.333333, 31.666667, 33.333333, 35.000000, 36.666667, 38.333…
## $ .percent_found  &lt;dbl&gt; 0.00000, 44.00000, 52.00000, 60.00000, 68.00000, 76.00000, 80.00000, 84.00000, 88.00000, 92.00000, 100.0…</code></pre>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::gain_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60randomForest%60-1.png" width="672" /></p>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 80
## Columns: 4
## $ .level      &lt;chr&gt; &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;, &quot;setosa&quot;…
## $ .threshold  &lt;dbl&gt; -Inf, 0.00, 0.01, 0.02, 0.06, 0.08, 0.09, 0.16, 0.71, 0.72, 0.74, 0.84, 0.92, 0.93, 0.96, 0.97, 0.99, 1.00, …
## $ specificity &lt;dbl&gt; 0.0000000, 0.0000000, 0.7428571, 0.8000000, 0.8285714, 0.8857143, 0.9428571, 0.9714286, 1.0000000, 1.0000000…
## $ sensitivity &lt;dbl&gt; 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.0000000, 0.9200000…</code></pre>
<pre class="r"><code>iris_rf_probs %&gt;% 
  yardstick::roc_curve(Species, .pred_setosa:.pred_virginica) %&gt;% 
  autoplot()</code></pre>
<p><img src="/courses/5-ML-for-Artists/3-Classification-with-Orange/Random-Forests_files/figure-html/Gain%20and%20ROC%20Curves%20%60randomForest%60-2.png" width="672" /></p>
</div>
<div id="iris-classifier-metrics" class="section level3">
<h3>Iris Classifier: Metrics</h3>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_setosa     &lt;dbl&gt; 0.890134921, 0.944750000, 0.953825397, 0.987750000, 0.978861111, 0.982750000, 0.890134921, 0.915134921,…
## $ .pred_versicolor &lt;dbl&gt; 0.102753968, 0.040638889, 0.036285714, 0.008888889, 0.016388889, 0.011388889, 0.102753968, 0.080253968,…
## $ .pred_virginica  &lt;dbl&gt; 0.007111111, 0.014611111, 0.009888889, 0.003361111, 0.004750000, 0.005861111, 0.007111111, 0.004611111,…
## $ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa,…
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa,…</code></pre>
<pre class="r"><code>predict(iris_ranger, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)</code></pre>
<pre><code>## # A tibble: 4 × 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy    multiclass     0.967
## 2 kap         multiclass     0.949
## 3 mn_log_loss multiclass     0.205
## 4 roc_auc     hand_till      0.986</code></pre>
<pre class="r"><code># And for the `randomForest`method

predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  glimpse()</code></pre>
<pre><code>## Rows: 60
## Columns: 5
## $ .pred_setosa     &lt;dbl&gt; 0.92, 0.97, 0.99, 1.00, 1.00, 1.00, 0.93, 0.96, 0.74, 0.99, 0.72, 1.00, 1.00, 0.97, 0.84, 0.96, 0.71, 1…
## $ .pred_versicolor &lt;dbl&gt; 0.07, 0.03, 0.01, 0.00, 0.00, 0.00, 0.06, 0.04, 0.25, 0.01, 0.28, 0.00, 0.00, 0.03, 0.16, 0.04, 0.29, 0…
## $ .pred_virginica  &lt;dbl&gt; 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.01, 0.00, 0.01, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0…
## $ .pred_class      &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa,…
## $ Species          &lt;fct&gt; setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa, setosa,…</code></pre>
<pre class="r"><code>predict(iris_rf, iris_testing_baked, type = &quot;prob&quot;) %&gt;% 
  bind_cols(predict(iris_ranger,iris_testing_baked)) %&gt;% 
  bind_cols(select(iris_testing_baked,Species)) %&gt;% 
  yardstick::metrics(data = ., truth = Species, estimate = .pred_class, ... = .pred_setosa:.pred_virginica)</code></pre>
<pre><code>## # A tibble: 4 × 3
##   .metric     .estimator .estimate
##   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;
## 1 accuracy    multiclass     0.967
## 2 kap         multiclass     0.949
## 3 mn_log_loss multiclass     0.200
## 4 roc_auc     hand_till      0.980</code></pre>
</div>
</div>
