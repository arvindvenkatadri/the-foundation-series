---
lastmod: "`r Sys.Date()`"
title: "\U0001F46A Populations, Samples, Statistics, and Inference"
author: "Arvind Venkatadri"
date: 12/Jan/2023
external_link: ""
weight: 20
type: book
df_print: paged
code_show: hide
code_download: TRUE
image:
  caption: Photo by <a href="https://unsplash.com/@lanirudhreddy?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">ANIRUDH</a> on <a href="https://unsplash.com/s/photos/matrix?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText">Unsplash</a>
  focal_point: Smart
links:
- icon: circle
  icon_pack: fas
  name: Orange Tutorial
  url: "labs/Data-Analytics/sampling-CLT.ows"
- icon: sun
  icon_pack: fas
  name: Radiant
  url: "labs/Data-Analytics/sampling-CLT.rda"
- icon: r-project
  icon_pack: fab
  name: R Tutorial
  url: "labs/Data-Analytics/sampling.html"
slides: 
subtitle: "How much Data does a Man need?"
tags:
- Sampling
- Central Limit Theorem
- Standard Error
url_code: ""
url_pdf: ""
url_slides: ""
url_video: ""
---


```{r setup, include = FALSE}
set.seed(123456)
knitr::opts_chunk$set(tidy.opts=list(width.cutoff=80),warning = FALSE)


library(tidyverse)
library(mosaic) # Our workhorse for stats, sampling
library(ggformula) # Formula interface for graphs

# load the NHANES data library
library(NHANES)

library(infer) # tidy workflow for statistical inference
library(gt) # Create tidy tables to report data
library(cowplot) # ggplot themes and stacking of plots


```

```{r,echo=FALSE, eval = FALSE, fig.align='center',fig.alt="Photo by Anirudh on unsplash"}
knitr::include_graphics("featured.jpg")

```

## What is a Population?

A *population* is a collection of individuals or observations we are
interested in. This is also commonly denoted as a study population. We
mathematically denote the population's size using upper-case `N`.

A *population parameter* is some numerical summary about the population
that is unknown but you wish you knew. For example, when this quantity
is a mean like the average height of all Bangaloreans, the population
parameter of interest is the population mean.

A *census* is an exhaustive enumeration or counting of all N individuals
in the population. We do this in order to compute the population
parameter's value exactly. Of note is that as the number N of
individuals in our population increases, conducting a census gets more
expensive (in terms of time, energy, and money).

## What is a Sample?

Sampling is the act of collecting a sample from the population, which we
generally do when we can't perform a census. We mathematically denote
the sample size using lower case `n`, as opposed to upper case N which
denotes the population's size. Typically the sample size n is much
smaller than the population size N. Thus sampling is a much cheaper
alternative than performing a census.

A **sample statistic**, also known as a *point estimate*, is a summary
statistic like a mean or standard deviation that is computed from a
sample.

## Why do we sample?

Because we cannot conduct a census ( not always ) --- and sometimes we
won't even know how big the population is --- we take samples. And we
*still* want to do useful work for/with the population, after
*estimating its parameters, an act of generalizing* from sample to
population. So the question is, **can we estimate useful parameters of
the population, using just samples? Can point estimates serve as useful
guides to population parameters?**

This act of generalizing from sample to population is at the heart of
**statistical inference**.

> NOTE: there is an *alliterative mnemonic* here: **S**amples have
> **S**tatistics; **P**opulations have **P**arameters.

## Sampling

We will first execute some samples from a known dataset. We load up the
NHANES dataset and inspect it.

```{r}

data("NHANES")
mosaic::inspect(NHANES)

```

Let us create a NHANES dataset without duplicated IDs and only adults:

```{r}

NHANES <-
  NHANES %>%
  distinct(ID, .keep_all = TRUE) 

#create a dataset of only adults
NHANES_adult <- 
  NHANES %>%
  filter(Age >= 18) %>%
  drop_na(Height)

glimpse(NHANES_adult)

```

For now, we will **treat** this dataset as our **Population**. So each
variable in the dataset is a population for that particular
quantity/category, with appropriate *population parameters* such as
means, sd-s, and proportions. Let us calculate the **population
parameters** for the `Height` data:

```{r}

pop_mean_height <- mean(~ Height, data = NHANES_adult)
pop_sd_height <- sd(~ Height, data = NHANES_adult)

pop_mean_height
pop_sd_height

```

### One Sample

Now, we will sample **ONCE** from the NHANES `Height` variable. Let us
take a sample of `sample size` 50. We will compare **sample statistics**
with **population parameters** on the basis of this ONE sample of 50:

```{r, fig.cap="Single-Sample Mean and Population Mean", fig.id=TRUE}

sample_height <- sample(NHANES_adult, size = 50) %>% select(Height)
sample_height

sample_mean_height <- mean(~ Height, data = sample_height)
sample_mean_height

# Plotting the histogram of this sample
sample_height %>% gf_histogram(~ Height) %>% 
  gf_vline(xintercept = sample_mean_height, color = "red") %>% 
  gf_vline(xintercept = pop_mean_height, colour = "blue") %>% 
  gf_text(1 ~ (pop_mean_height + 5), label = "Population Mean Height", color = "blue") %>% 
  gf_text(2 ~ (sample_mean_height-5), label = "Sample Mean Height", color = "red")

```

### 500 Samples

OK, so the `sample_mean_height` is not too far from the
`pop_mean_height`. Is this always true? Let us check: we will create 500
samples each of size 50. And calculate their mean as the *sample
statistic*, giving us a dataframe containing 5000 `sample means`. We
will then compare if these 500 means are close to the `pop_mean_height`:

```{r,fig.cap="Multiple Sample-Means and Population Mean", fig.id=TRUE}

sample_height_500 <- do(500) * {
  sample(NHANES_adult, size = 50) %>%
    select(Height) %>%
    summarise(
      sample_mean_500 = mean(Height),
      sample_min_500 = min(Height),
      sample_max_500 = max(Height))
}

head(sample_height_500)
dim(sample_height_500)

sample_height_500 %>%
  gf_point(.index ~ sample_mean_500, color = "red") %>%
  gf_segment(
    .index + .index ~ sample_min_500 + sample_max_500,
    color = "red",
    size = 0.3,
    alpha = 0.3,
    ylab = "Sample Index (1-500)",
    xlab = "Sample Means"
  ) %>%
  gf_vline(xintercept = ~ pop_mean_height, color = "blue") %>%
  gf_label(-15 ~ pop_mean_height, label = "Population Mean", color = "blue")

```

The `sample_mean`s (red dots), are themselves random because the samples
are random, of course. It appears that they are generally in the
vicinity of the `pop_mean` (blue line).

### Distribution of Sample-Means

Since the **sample-means** are themselves random variables, let's plot
the **distribution** of these 5000 sample-means themselves, called a **a
distribution of sample-means**.

> NOTE: this **a distribution of sample-means** will *itself* have a
> mean and standard deviation. Do not get confused ;-D

We will also plot the position of the population mean `pop_mean_height`
parameter, the means of the `Height` variable.

```{r,Sampling-Mean-Distribution, fig.width=8,fig.height=4,out.height='50%', fig.cap="Sampling Mean Distribution", fig.id=TRUE}

sample_height_500 %>% gf_dhistogram(~ sample_mean_500) %>% 
  gf_vline(xintercept = pop_mean_height, color = "blue") %>% 
   gf_label(0.01 ~ pop_mean_height, label = "Population Mean", color = "blue")

```

How does this **distribution of sample-means** compare with that of the
overall distribution of the population heights?

```{r, fig.cap="Sampling Means and Population Distributions", fig.id=TRUE}

sample_height_500 %>% gf_dhistogram(~ sample_mean_500,bins = 50,fill = "red") %>% 
  gf_vline(xintercept = pop_mean_height, color = "blue") %>% 
   gf_label(0.01 ~ pop_mean_height, label = "Population Mean", color = "blue") %>% 

  ## Add the population histogram
  gf_histogram(~ Height, data = NHANES_adult, alpha = 0.2, fill = "blue", bins = 50) %>% 
  gf_label(0.025 ~ (pop_mean_height + 20), label = "Population Distribution of Height", color = "blue") %>% 
    gf_label(0.25 ~ (pop_mean_height + 17), label = "Distribution of 500 Height Sample Means", color = "red")


```

### Central limit theorem

We see in the Figure above that

-   the *distribution of sample-means* is centered around mean =
    `pop_mean`.
-   That the standard deviation of the *distribution of sample means* is
    less than that of the original population. But exactly what is it?
-   And what is the kind of distribution?

One more experiment.

Now let's repeatedly sample `Height` and compute the sample mean, and
look at the resulting histograms and Q-Q plots. ( Q-Q plots check
whether a certain distribution is close to being normal or not.)

We will use sample sizes of `c(16, 32, 64, 128)` and generate 1000
samples each time, take the means and plot these 1000 means:

```{r}
set.seed(12345)


samples_height_16 <- do(1000) * mean(resample(NHANES_adult$Height, size = 16))
samples_height_32 <- do(1000) * mean(resample(NHANES_adult$Height, size = 32))
samples_height_64 <- do(1000) * mean(resample(NHANES_adult$Height, size = 64))
samples_height_128 <- do(1000) * mean(resample(NHANES_adult$Height, size = 128))

# Quick Check
head(samples_height_16)

### do(1000,) * mean(resample(NHANES_adult$Height, size = 16)) produces a data frame with a variable named mean.
###

```

Now let's create separate Q-Q plots for the different sample sizes.

```{r,echo=FALSE, warning=FALSE}


# Now let's create separate Q-Q plots for the different sample sizes.
#
p1 <- gf_qq( ~ mean,data = samples_height_16,title = "N = 16", color = "cornsilk") %>%
  gf_qqline()

p2 <-gf_qq( ~ mean,data = samples_height_32,title = "N = 32", color = "sienna") %>%
  gf_qqline()

p3 <-gf_qq( ~ mean,data = samples_height_32,title = "N = 64", color = "tomato2") %>%
  gf_qqline()

p4 <-gf_qq( ~ mean,data = samples_height_128,title = "N = 128", color = "violetred") %>%
  gf_qqline()

cowplot::plot_grid(p1, p2, p3, p4)

```

Let us plot their individual histograms to compare them:

```{r, echo=FALSE}

# Let us overlay their individual histograms to compare them:
p5 <- gf_dhistogram(~ mean,
              data = samples_height_16,
              color = "black",
              fill = "cornsilk",title = "N = 16") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-0.01 ~ pop_mean_height, label = "Population Mean", color = "blue")

p6 <- gf_dhistogram(~ mean,
              data = samples_height_32 ,
              color = "black",
              fill = "sienna",title = "N = 32") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.01 ~ pop_mean_height, label = "Population Mean", color = "blue")

p7 <- gf_dhistogram(
  ~ mean,
  data = samples_height_64 ,
  na.rm = TRUE,
  color = "black",
  fill = "tomato2",title = "N = 64") %>%
  gf_fitdistr() %>%
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.01 ~ pop_mean_height, label = "Population Mean", color = "blue")

p8 <- gf_dhistogram(
  ~ mean,
  data = samples_height_128 ,
  na.rm = TRUE,
  color = "black",
  fill = "violetred",title = "N = 128") %>% 
  gf_fitdistr() %>% 
  gf_vline(xintercept = pop_mean_height,
         color = "blue") %>%
  gf_label(-.01 ~ pop_mean_height, label = "Population Mean", color = "blue")

cowplot::plot_grid(p5,p6,p7,p8)


```

And if we overlay the histograms:

```{r, echo=FALSE}
gf_dhistogram(~ mean,
              data = samples_height_16,
              color = "black",
              fill = "cornsilk",alpha = .5) %>% 

gf_dhistogram(~ mean,
              data = samples_height_32,
              color = "black",
              fill = "sienna",alpha = 0.5) %>% 

gf_dhistogram(~ mean,
              data = samples_height_64,
              color = "black",
              fill = "tomato2",alpha = 0.5) %>% 

gf_dhistogram(~ mean,
              data = samples_height_128,
              color = "black",
              fill = "violetred",alpha = 0.5) %>% 

gf_fitdistr(
  ~ mean,
  data = samples_height_16,
  color = "cornsilk",size = 2
) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_32 ,
    color = "sienna",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_64 ,
    na.rm = TRUE,
    color = "tomato2",size = 2
  ) %>%
  
  gf_fitdistr(
    ~ mean,
    data = samples_height_128 ,
    na.rm = TRUE,
    color = "violetred", size = 2
  ) %>%
  
  gf_vline(xintercept = pop_mean_height,
           color = "blue") %>%
  gf_label(-.02 ~ pop_mean_height, label = "Population Mean", color = "blue") %>% gf_theme(ggplot2::theme_minimal())

```

This shows that the results become more normally distributed (i.e.
following the straight line) as the samples get larger. Hence we learn
that:

-   the sample-means are normally distributed around the *population
    mean*. This is because when we sample from the population, many
    values will be close to the *population mean*, and values far away
    from the mean will be increasingly scarce.

```{r,results='hold'}
mean(~ mean, data  = samples_height_16)
mean(~ mean, data  = samples_height_32)
mean(~ mean, data  = samples_height_64)
mean(~ mean, data  = samples_height_128)
pop_mean_height

```

-   the sample-means become "more normally distributed" with sample
    length, as shown by the (small but definite) improvements in the Q-Q
    plots with sample-size.

-   the sample-mean distributions narrow with sample length.

**This is regardless of the distribution of the *population* itself**. (
The `Height` variable seems to be normally distributed at population
level. We will try other non-normal population variables as an
exercise). This is the **Central Limit Theorem (CLT)**.

`r blogdown::shortcode("youtube","_YOr_yYPytM" )`


As we saw in the figure above, the standard deviations of the
*sample-mean distributions reduce with sample size*. In fact their SDs
are defined by:

`sd = pop_sd/sqrt(sample_size)` where sample-size here is one of
`c(16,32,64,128)`

```{r,results='hold'}

sd(~ mean, data  = samples_height_16)
sd(~ mean, data  = samples_height_32)
sd(~ mean, data  = samples_height_64)
sd(~ mean, data  = samples_height_128)

```

The standard deviation of the **sample-mean distribution** is called the
**Standard Error**. This statistic derived from the sample, will help us
infer our population parameters with a precise estimate of the
*uncertainty* involved.

$$
Standard\ Error\ {\pmb {se} = \frac{population\ sd}{\sqrt[]{sample\ size}}}\\\\
\\
\pmb {se} = \frac{\sigma}{\sqrt[]{n}}\\
$$

In our sampling experiments, the Standard Errors evaluate to:

```{r,results='hold'}

pop_sd_height <- sd(~ Height, data = NHANES_adult)

pop_sd_height/sqrt(16)
pop_sd_height/sqrt(32)
pop_sd_height/sqrt(64)
pop_sd_height/sqrt(128)

```

As seen, these are identical to the Standard Deviations of the
individual sample-mean distributions.

## Confidence intervals

When we work with samples, we want to be able to speak with a certain
degree of confidence about the **population mean**, based on the
evaluation of **one** sample mean,not a whole large number of them. Give
that sample-means are normally distributed around the **population
means**, we can say that $68%$ of *all possible sample-mean* lie within
\$ +/- 1 SE \$ of the *population mean*; and further that $95%$ of of
*all possible sample-mean* lie within \$ +/- 1.5\* SE \$ of the
*population mean*.

These two intervals **[sample-mean +/- SE]** and **[sample-mean +/-
1.5SE]** are called the **confidence intervals** for the population
mean, at levels 68% and 95% respectively.

Thus if we want to estimate a *population mean*:\\
- we take one sample of size $n$ from the population\
- we calculate the sample-mean - we calculate the sample-sd\
- We calculate the Standard Error as $\frac{sample-sd}{\sqrt[]{n}}$\
- We calculate 95% confidence intervals for the *population mean* based on
the formula above.\

Now that we have our basics ready, it is time to play with a dataset in
the different tools at our disposal.

## A Sampling Workflow in Orange

```{r, echo=FALSE}
library(downloadthis)
download_file(
  path = "files/sampling-CLT.ows",
  output_extension = ".ows",
  output_name = "sampling-CLT",
  button_label = "Download Orange Workflow",
  button_type = "warning",
  has_icon = TRUE,
  icon = "fa fa-save",
  self_contained = FALSE
)
```

## A Sampling Workflow in Radiant

```{r, echo=FALSE}
library(downloadthis)
download_file(
  path = "files/sampling-CLT.rda",
  output_extension = ".",
  output_name = "sampling-CLT",
  button_label = "Download Radiant Workflow",
  button_type = "success",
  has_icon = TRUE,
  icon = "fa fa-person-rays",
  self_contained = FALSE
)
```


## A Sampling Workflow in R

```{r, echo=FALSE}
library(downloadthis)
download_file(
  path = "files/sampling.Rmd",
  output_extension = ".Rmd",
  output_name = "sampling",
  button_label = "Download RMarkdown Workflow",
  button_type = "info",
  has_icon = TRUE,
  icon = "fa fa-r-project",
  self_contained = FALSE
)
```

## References

1.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine,
    *OpenIntro Statistics*. <https://www.openintro.org/book/os/>

2.  Stats Test Wizard.
    <https://www.socscistatistics.com/tests/what_stats_test_wizard.aspx>

3.  Diez, David M & Barr, Christopher D & Çetinkaya-Rundel, Mine:
    *OpenIntro Statistics*. Available online
    <https://www.openintro.org/book/os/>

4.  Måns Thulin, *Modern Statistics with R: From wrangling and exploring
    data to inference and predictive modelling*
    <http://www.modernstatisticswithr.com/>

5.  Jonas Kristoffer Lindeløv, Common statistical tests are linear
    models (or: how to teach stats)
    <https://lindeloev.github.io/tests-as-linear/>

6.  CheatSheet
    <https://lindeloev.github.io/tests-as-linear/linear_tests_cheat_sheet.pdf>

7.  Common statistical tests are linear models: a work through by Steve
    Doogue <https://steverxd.github.io/Stat_tests/>

8.  Jeffrey Walker "Elements of Statistical Modeling for Experimental
    Biology".
    <https://www.middleprofessor.com/files/applied-biostatistics_bookdown/_book/>
